<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Chen Research Group——Welcome to Chen's lab website</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/UCSC_icon.png" rel="icon">
  <link href="assets/img/UCSC_icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/fontawesome-free/css/all.min.css" rel="stylesheet">
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Lato:100,300,400,700,900" rel="stylesheet">
  <link rel="stylesheet" type="text/css" media="screen,print" href="assets/css_pub/style.css" />
<!--   <link href="assets/css_pub/bootstrap.min.css" rel="stylesheet" media="screen" /> -->
  <link rel="icon" type="image/png" href="./images/logos/princeton.png">
  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Medilab - v4.7.1
  * Template URL: https://bootstrapmade.com/medilab-free-medical-bootstrap-theme/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Top Bar ======= -->
  <div id="topbar" class="d-flex align-items-center fixed-top">
    <div class="container d-flex justify-content-between">
      <div class="contact-info d-flex align-items-center">
      </div>
       <!-- 
      <div class="d-none d-lg-flex social-links align-items-center">
        <a href="opening.html" class="envelope"><i class="bi-envelope"></i></a>
      </div>-->
    </div>
  </div>








  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex align-items-center">

      <h1 class="logo me-auto"><a href="index.html">CHEN's lab</a></h1>
      <nav id="navbar" class="navbar order-last order-lg-0">
        <ul>
          <li><a class="nav-link scrollto" href="index.html">Home</a></li>
          <li><a class="nav-link scrollto" href="people.html">People</a></li>
          <li><a class="nav-link scrollto" href="https://github.com/UCSC-VLAA">Research</a></li>
          <li><a class="nav-link scrollto active" href="publications.html">Publications</a></li>
          <li><a class="nav-link scrollto" href="index.html#news">News</a></li>
          <li><a class="nav-link scrollto" href="opening.html">Openings</a></li>
          <li><a class="nav-link scrollto" href="index.html#contact">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->


    </div>
  </header><!-- End Header -->

    <br>
    <br>
    <br>
    <br>
    <br>
    <br>     
    <br>
    <br>
   

    <div class="section-title">
      <h2>Publications</h2>
    </div>       
    
<script>
function copy(dest, source) {
  if(dest.source == source) {
    dest.innerHTML = "";
    dest.source = null;
    dest.style.width="0px";
    dest.style.border = "";
    dest.style.padding = "0px";
  }
  else {
    dest.innerHTML = source.innerHTML;
    dest.source = source;
    dest.style.width = "800px";
    dest.style.padding = "10px";
    dest.style.border = "2px dotted gray";
    dest.style.background = "#F5F5F5";
    dest.style.margin = "10px";
  }
  dest.blur();
}
</script>

<div class="container">
<!-- <h1> Papers</h1> -->
<br>
<!-- <p>(*: equal contribution)</p> -->

<details close>
    <summary><font size="5">Pre-print</font></summary>
    <script>
        paper_count = 0

        function add_paper(title, authors, conference, link, bib, abstract, arxiv_link, code, press, slides, talk, msg) {
            list_entry = "<li style=\"font-size:18px\">"
            if (link != null)
                list_entry += "<a href=\"" + link + "\">"
            list_entry += "<b>" + title + "</b>"
            if (link != null)
                list_entry += "</a>"
            list_entry += "<br>" + authors + ".<br>" 
            if (conference != null)
                list_entry+= conference + ".</li>"
            if (bib != null) {
                list_entry += "<div id=\"bib" + paper_count + "\" style=\"display:none\">" + bib + "</div>"
                list_entry += "<a href=\"javascript:copy(div" + paper_count + ",bib" + paper_count + ")\"> <span class=\"label label-success\">bib</span></a>"
            }

            if (abstract != null) {
                list_entry += "<div id=\"abstract" + paper_count + "\" style=\"display:none\">" + abstract + "</div>"
                list_entry += "<a href=\"javascript:copy(div" + paper_count + ",abstract" + paper_count + ")\"> <span class=\"label label-warning\">abstract</span></a>"
            }
            if (arxiv_link != null)
                list_entry += " <a href=\"" + arxiv_link + "\"><span class=\"label label-primary\">arxiv</span></a>"

            if (code != null)
                list_entry += " <a href=\"" + code + "\"><span class=\"label label-danger\">code/models</span></a>"

            if (press != null)
                list_entry += " <a href=\"" + press + "\"><span class=\"label label-success\">press</span></a>"

            if (slides != null)
                list_entry += " <a href=\"" + slides + "\"><span class=\"label label-info\">slides/poster</span></a>"

            if (talk != null)
                list_entry += " <a href=\"" + talk + "\"><span class=\"label label-default\">talk</span></a>"

            list_entry += "<br>"

            if (msg != null)
                list_entry += "<i>" + msg + "</i>"

            list_entry += "<div id=\"div" + paper_count + "\" style=\"font-size:15px\"></div><br>"

            document.write(list_entry)

            paper_count += 1
        }

        // document.write("<h2>Preprint</h2>")
        // document.write("<ul>")
        document.write("</ul>")
        // document.write("<h2>Preprint</h2>")
        document.write("<ul><br>")



        add_paper("Charge Asymmetry Suppresses Coarsening Dynamics in Polyelectrolyte Complex Coacervation",
          "Shensheng CHEN，Zhen-Gang Wang",
          null,
          "https://link.aps.org/doi/10.1103/PhysRevLett.131.218201",
          "@article{ren2024mvar,<br>" +
          "&nbsp;&nbsp;&nbsp;title   = {M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality Image Generation},<br>" +
          "&nbsp;&nbsp;&nbsp;author  = {Sucheng Ren, Yaodong Yu, Nataniel Ruiz, Feng Wang, Alan Yuille, Cihang Xie},<br>" +
          "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2411.10433},<br>" +
          "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "There exists recent work in computer vision, named VAR, that proposes a new autoregressive paradigm for image generation. Diverging from the vanilla next-token prediction, VAR structurally reformulates the image generation into a coarse to fine next-scale prediction. In this paper, we show that this scale-wise autoregressive framework can be effectively decoupled into intra-scale modeling, which captures local spatial dependencies within each scale, and inter-scale modeling, which models cross-scale relationships progressively from coarse-to-fine scales. This decoupling structure allows to rebuild VAR in a more computationally efficient manner. Specifically, for intra-scale modeling -- crucial for generating high-fidelity images -- we retain the original bidirectional self-attention design to ensure comprehensive modeling; for inter-scale modeling, which semantically connects different scales but is computationally intensive, we apply linear-complexity mechanisms like Mamba to substantially reduce computational overhead. We term this new framework M-VAR. Extensive experiments demonstrate that our method outperforms existing models in both image quality and generation speed. For example, our 1.5B model, with fewer parameters and faster inference speed, outperforms the largest VAR-d30-2B. Moreover, our largest model M-VAR-d32 impressively registers 1.78 FID on ImageNet 256x256 and outperforms the prior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion models LDM/DiT by 1.82/0.49, respectively. Code is available at https://github.com/OliverRensu/MVAR.",
          "https://arxiv.org/abs/2411.10433",
          "https://github.com/OliverRensu/MVAR"
          )


        add_paper("AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation",
          "Zijun Wang, Haoqin Tu, Jieru Mei, Bingchen Zhao, Yisen Wang, Cihang Xie",
          null,
          "https://arxiv.org/abs/2410.09040",
          "@article{wang2024attngcg,<br>" +
          "&nbsp;&nbsp;&nbsp;title   = {AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation},<br>" +
          "&nbsp;&nbsp;&nbsp;author  = {Zijun Wang, Haoqin Tu, Jieru Mei, Bingchen Zhao, Yisen Wang, Cihang Xie},<br>" +
          "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2410.09040},<br>" +
          "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, focusing specifically on the optimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe a positive correlation between the effectiveness of attacks and the internal behaviors of the models. For instance, attacks tend to be less effective when models pay more attention to system prompts designed to ensure LLM safety alignment. Building on this discovery, we introduce an enhanced method that manipulates models' attention scores to facilitate LLM jailbreaking, which we term AttnGCG. Empirically, AttnGCG shows consistent improvements in attack efficacy across diverse LLMs, achieving an average increase of ~7% in the Llama-2 series and ~10% in the Gemma series. Our strategy also demonstrates robust attack transferability against both unseen harmful goals and black-box LLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score visualization is more interpretable, allowing us to gain better insights into how our targeted attention manipulation facilitates more effective jailbreaking. We release the code at https://github.com/UCSC-VLAA/AttnGCG-attack.",
          "https://arxiv.org/abs/2410.09040",
          "https://github.com/UCSC-VLAA/AttnGCG-attack"
          )


        add_paper("Causal Image Modeling for Efficient Visual Understanding",
          "Feng Wang, Timing Yang, Yaodong Yu, Sucheng Ren, Guoyizhe Wei, Angtian Wang, Wei Shao, Yuyin Zhou, Alan Yuille, Cihang Xie",
          null,
          "https://arxiv.org/abs/2410.07599",
          "@article{wang2024causalimagemodeling,<br>" +
          "&nbsp;&nbsp;&nbsp;title   = {Causal Image Modeling for Efficient Visual Understanding},<br>" +
          "&nbsp;&nbsp;&nbsp;author  = {Feng Wang, Timing Yang, Yaodong Yu, Sucheng Ren, Guoyizhe Wei, Angtian Wang, Wei Shao, Yuyin Zhou, Alan Yuille, Cihang Xie},<br>" +
          "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2410.07599},<br>" +
          "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "In this work, we present a comprehensive analysis of causal image modeling and introduce the Adventurer series models where we treat images as sequences of patch tokens and employ uni-directional language models to learn visual representations. This modeling paradigm allows us to process images in a recurrent formulation with linear complexity relative to the sequence length, which can effectively address the memory and computation explosion issues posed by high-resolution and fine-grained images. In detail, we introduce two simple designs that seamlessly integrate image inputs into the causal inference framework: a global pooling token placed at the beginning of the sequence and a flipping operation between every two layers. Extensive empirical studies demonstrate the significant efficiency and effectiveness of this causal image modeling paradigm. For example, our base-sized Adventurer model attains a competitive test accuracy of 84.0% on the standard ImageNet-1k benchmark with 216 images/s training throughput, which is 5.3 times more efficient than vision transformers to achieve the same result.",
          "https://arxiv.org/abs/2410.07599",
          null
          )

        add_paper("Story-Adapter: A Training-free Iterative Framework for Long Story Visualization",
          "Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, Yuyin Zhou",
          null,
          "https://arxiv.org/abs/2410.06244",
          "@article{mao2024storyadapter,<br>" +
          "&nbsp;&nbsp;&nbsp;title   = {Story-Adapter: A Training-free Iterative Framework for Long Story Visualization},<br>" +
          "&nbsp;&nbsp;&nbsp;author  = {Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, Yuyin Zhou},<br>" +
          "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2410.06244},<br>" +
          "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference cross-attention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios. The project page and associated code can be accessed via https://jwmao1.github.io/storyadapter.",
          "https://arxiv.org/abs/2410.06244",
          "https://jwmao1.github.io/storyadapter"
          )


        add_paper("A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?",
            "Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, Yuyin Zhou",
            null,
            "https://arxiv.org/abs/2409.15277",
            "@article{xie2024preliminarystudyo1medicine,<br>" +
            "&nbsp;&nbsp;&nbsp;title   = {A Preliminary Study of o1 in Medicine: Are We Closer to an AI Doctor?},<br>" +
            "&nbsp;&nbsp;&nbsp;author  = {Yunfei Xie, Juncheng Wu, Haoqin Tu, Siwei Yang, Bingchen Zhao, Yongshuo Zong, Qiao Jin, Cihang Xie, Yuyin Zhou},<br>" +
            "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2409.15277},<br>" +
            "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
            "Large language models (LLMs) have exhibited remarkable capabilities across various domains and tasks, pushing the boundaries of our knowledge in learning and cognition. The latest model, OpenAI's o1, stands out as the first LLM with an internalized chain-of-thought technique using reinforcement learning strategies. While it has demonstrated surprisingly strong capabilities on various general language tasks, its performance in specialized fields such as medicine remains unknown. To this end, this report provides a comprehensive exploration of o1 on different medical scenarios, examining 3 key aspects: understanding, reasoning, and multilinguality. Specifically, our evaluation encompasses 6 tasks using data from 37 medical datasets, including two newly constructed and more challenging question-answering (QA) tasks based on professional medical quizzes from the New England Journal of Medicine (NEJM) and The Lancet. These datasets offer greater clinical relevance compared to standard medical QA benchmarks such as MedQA, translating more effectively into real-world clinical utility. Our analysis of o1 suggests that the enhanced reasoning ability of LLMs may (significantly) benefit their capability to understand various medical instructions and reason through complex clinical scenarios. Notably, o1 surpasses the previous GPT-4 in accuracy by an average of 6.2% and 6.6% across 19 datasets and two newly created complex QA scenarios. But meanwhile, we identify several weaknesses in both the model capability and the existing evaluation protocols, including hallucination, inconsistent multilingual ability, and discrepant metrics for evaluation. We release our raw data and model outputs https://ucsc-vlaa.github.io/o1_medicine/ for future research.",
            "https://arxiv.org/abs/2409.15277",
            "https://ucsc-vlaa.github.io/o1_medicine/"
            )


        add_paper("Restorer: Removing Multi-Degradation with All-Axis Attention and Prompt Guidance",
          "Jiawei Mao, Juncheng Wu, Yuyin Zhou, Xuesong Yin, Yuanqi Chang",
          null,
          "https://arxiv.org/abs/2406.12587",
          "@article{mao2024restorer,<br>" +
          "&nbsp;&nbsp;&nbsp;title = {Restorer: Removing Multi-Degradation with All-Axis Attention and Prompt Guidance},<br>" +
          "&nbsp;&nbsp;&nbsp;author = {Mao, Jiawei and Wu, Juncheng and Zhou, Yuyin and Yin, Xuesong and Chang, Yuanqi},<br>" +
          "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2406.12587},<br>" +
          "&nbsp;&nbsp;&nbsp;year = {2024}<br>}",
          "There are many excellent solutions in image restoration. However, most methods require on training separate models to restore images with different types of degradation. Although existing all-in-one models effectively address multiple types of degradation simultaneously, their performance in real-world scenarios is still constrained by the task confusion problem. In this work, we attempt to address this issue by introducing Restorer, a novel Transformer-based allin-one image restoration model. To effectively address the complex degradation present in real-world images, we propose All-Axis Attention (AAA), a novel attention mechanism that simultaneously models long-range dependencies across both spatial and channel dimensions, capturing potential correlations along all axes. Additionally, we introduce textual prompts in Restorer to incorporate explicit task priors, enabling the removal of specific degradation types based on user instructions. By iterating over these prompts, Restorer can handle composite degradation in real-world scenarios without requiring additional training. Based on these designs, Restorer with one set of parameters demonstrates state-of-theart performance in multiple image restoration tasks compared to existing all-in-one and even single-task models. Additionally, Restorer is efficient during inference, suggesting the potential in real-world applications. Code will be available at https://github.com/Talented-Q/Restorer.",
          "https://arxiv.org/abs/2406.12587",
          "https://github.com/Talented-Q/Restorer."
        )


        add_paper("MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine",
        "Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou",
        null,
        "https://arxiv.org/abs/2408.02900",
        "@article{xie2024medtrinity,<br>" +
        "&nbsp;&nbsp;&nbsp;title   = {MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine},<br>" +
        "&nbsp;&nbsp;&nbsp;author  = {Yunfei Xie, Ce Zhou, Lang Gao, Juncheng Wu, Xianhang Li, Hong-Yu Zhou, Sheng Liu, Lei Xing, James Zou, Cihang Xie, Yuyin Zhou},<br>" +
        "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2408.02900},<br>" +
        "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
        "This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular texual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. This dataset can be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain.",
        "https://arxiv.org/abs/2408.02900",
        "https://yunfeixie233.github.io/MedTrinity-25M/"
        )

        add_paper("VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges",
          "Yuxuan Wang, Cihang Xie, Yang Liu, Zilong Zheng",
          null,
          "https://arxiv.org/abs/2409.01071",
          "@article{wang2024videollamb,<br>" +
          "&nbsp;&nbsp;&nbsp;title   = {VideoLLaMB: Long-context Video Understanding with Recurrent Memory Bridges},<br>" +
          "&nbsp;&nbsp;&nbsp;author  = {Yuxuan Wang, Cihang Xie, Yang Liu, Zilong Zheng},<br>" +
          "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2409.01071},<br>" +
          "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "Recent advancements in large-scale video-language models have shown significant potential for real-time planning and detailed interactions. However, their high computational demands and the scarcity of annotated datasets limit their practicality for academic researchers. In this work, we introduce VideoLLaMB, a novel framework that utilizes temporal memory tokens within bridge layers to allow for the encoding of entire video sequences alongside historical visual data, effectively preserving semantic continuity and enhancing model performance across various tasks. This approach includes recurrent memory tokens and a SceneTilling algorithm, which segments videos into independent semantic units to preserve semantic integrity. Empirically, VideoLLaMB significantly outstrips existing video-language models, demonstrating a 5.5 points improvement over its competitors across three VideoQA benchmarks, and 2.06 points on egocentric planning. Comprehensive results on the MVBench show that VideoLLaMB-7B achieves markedly better results than previous 7B models of same LLM. Remarkably, it maintains robust performance as PLLaVA even as video length increases up to 8 times. Besides, the frame retrieval results on our specialized Needle in a Video Haystack (NIAVH) benchmark, further validate VideoLLaMB's prowess in accurately identifying specific frames within lengthy videos. Our SceneTilling algorithm also enables the generation of streaming video captions directly, without necessitating additional training. In terms of efficiency, VideoLLaMB, trained on 16 frames, supports up to 320 frames on a single Nvidia A100 GPU with linear GPU memory scaling, ensuring both high performance and cost-effectiveness, thereby setting a new foundation for long-form video-language models in both academic and practical applications.",
          "https://arxiv.org/abs/2409.01071",
          "https://videollamb.github.io/"
          )

        
        add_paper("What If We Recaption Billions of Web Images with LLaMA-3",
          "Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, Cihang Xie",
          null,
          "https://arxiv.org/abs/2406.08478",
          "@article{li2024recaption,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {What If We Recaption Billions of Web Images with LLaMA-3},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Li, Xianhang and Tu, Haoqin and Hui, Mude and Wang, Zeyu and Zhao, Bingchen and Xiao, Junfei and Ren, Sucheng and Mei, Jieru and Liu, Qing and Zheng, Huangjie and Zhou, Yuyin and Xie, Cihang},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2406.08478},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "Web-crawled image-text pairs are inherently noisy. Prior studies demonstrate that semantically aligning and enriching textual descriptions of these pairs can significantly enhance model training across various vision-language tasks, particularly text-to-image generation. However, large-scale investigations in this area remain predominantly closed-source. Our paper aims to bridge this community effort, leveraging the powerful and \textit{open-sourced} LLaMA-3, a GPT-4 level LLM. Our recaptioning pipeline is simple: first, we fine-tune a LLaMA-3-8B powered LLaVA-1.5 and then employ it to recaption 1.3 billion images from the DataComp-1B dataset. Our empirical results confirm that this enhanced dataset, Recap-DataComp-1B, offers substantial benefits in training advanced vision-language models. For discriminative models like CLIP, we observe enhanced zero-shot performance in cross-modal retrieval tasks. For generative models like text-to-image Diffusion Transformers, the generated images exhibit a significant improvement in alignment with users' text instructions, especially in following complex queries. Our project page is https://www.haqtu.me/Recap-Datacomp-1B/",
          "https://arxiv.org/abs/2406.08478",
          "https://www.haqtu.me/Recap-Datacomp-1B/"
        )


        add_paper("Medical Vision Generalist: Unifying Medical Imaging Tasks in Context",
          "Sucheng Ren, Xiaoke Huang, Xianhang Li, Junfei Xiao, Jieru Mei, Zeyu Wang, Alan Yuille, Yuyin Zhou",
          null,
          "https://arxiv.org/abs/2406.05565",
          "@article{ren2024medicalvision,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {Medical Vision Generalist: Unifying Medical Imaging Tasks in Context},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Ren, Sucheng and Huang, Xiaoke and Li, Xianhang and Xiao, Junfei and Mei, Jieru and Wang, Zeyu and Yuille, Alan and Zhou, Yuyin},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2406.05565},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "This study presents Medical Vision Generalist (MVG), the first foundation model capable of handling various medical imaging tasks -- such as cross-modal synthesis, image segmentation, denoising, and inpainting -- within a unified image-to-image generation framework. Specifically, MVG employs an in-context generation strategy that standardizes the handling of inputs and outputs as images. By treating these tasks as an image generation process conditioned on prompt image-label pairs and input images, this approach enables a flexible unification of various tasks, even those spanning different modalities and datasets. To capitalize on both local and global context, we design a hybrid method combining masked image modeling with autoregressive training for conditional image generation. This hybrid approach yields the most robust performance across all involved medical imaging tasks. To rigorously evaluate MVG's capabilities, we curated the first comprehensive generalist medical vision benchmark, comprising 13 datasets and spanning four imaging modalities (CT, MRI, X-ray, and micro-ultrasound). Our results consistently establish MVG's superior performance, outperforming existing vision generalists, such as Painter and LVM. Furthermore, MVG exhibits strong scalability, with its performance demonstrably improving when trained on a more diverse set of tasks, and can be effectively adapted to unseen datasets with only minimal task-specific samples. The code is available at https://github.com/OliverRensu/MVG.",
          "https://arxiv.org/abs/2406.05565",
          "https://github.com/OliverRensu/MVG"
        )


        add_paper("Fast-DDPM: Fast Denoising Diffusion Probabilistic Models for Medical Image-to-Image Generation",
          "Hongxu Jiang, Muhammad Imran, Linhai Ma, Teng Zhang, Yuyin Zhou, Muxuan Liang, Kuang Gong, Wei Shao",
          null,
          "https://arxiv.org/abs/2405.14802",
          "@article{jiang2024fastddpm,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {Fast Denoising Diffusion Probabilistic Models for Medical Image-to-Image Generation},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Jiang, Hongxu and Imran, Muhammad and Ma, Linhai and Zhang, Teng and Zhou, Yuyin and Liang, Muxuan and Gong, Kuang and Shao, Wei},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2405.14802},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented success in computer vision. However, they remain underutilized in medical imaging, a field crucial for disease diagnosis and treatment planning. This is primarily due to the high computational cost associated with (1) the use of large number of time steps (e.g., 1,000) in diffusion processes and (2) the increased dimensionality of medical images, which are often 3D or 4D. Training a diffusion model on medical images typically takes days to weeks, while sampling each image volume takes minutes to hours. To address this challenge, we introduce Fast-DDPM, a simple yet effective approach capable of improving training speed, sampling speed, and generation quality simultaneously. Unlike DDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains and samples using only 10 time steps. The key to our method lies in aligning the training and sampling procedures. We introduced two efficient noise schedulers with 10 time steps: one with uniform time step sampling and another with non-uniform sampling. We evaluated Fast-DDPM across three medical image-to-image generation tasks: multi-image super-resolution, image denoising, and image-to-image translation. Fast-DDPM outperformed DDPM and current state-of-the-art methods based on convolutional networks and generative adversarial networks in all tasks. Additionally, Fast-DDPM reduced training time by a factor of 5 and sampling time by a factor of 100 compared to DDPM. Our code is publicly available at: https://github.com/mirthAI/Fast-DDPM.",
          "https://arxiv.org/abs/2405.14802",
          "https://github.com/mirthAI/Fast-DDPM"
        )

        add_paper("VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models",
          "Yuxuan Wang, Yueqian Wang, Dongyan Zhao, Cihang Xie, Zilong Zheng",
          null,
          "https://arxiv.org/abs/2406.16338",
          "@article{wang2025videohallucer,<br>" +
          "&nbsp;&nbsp;&nbsp;title   = {VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models},<br>" +
          "&nbsp;&nbsp;&nbsp;author  = {Wang, Yuxuan and Wang, Yueqian and Zhao, Dongyan and Xie, Cihang and Zheng, Zilong},<br>" +
          "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2406.16338},<br>" +
          "&nbsp;&nbsp;&nbsp;year    = {2024}<br>",
          "Recent advancements in Multimodal Large Language Models (MLLMs) have extended their capabilities to video understanding. Yet, these models are often plagued by 'hallucinations', where irrelevant or nonsensical content is generated, deviating from the actual video context. This work introduces VideoHallucer, the first comprehensive benchmark for hallucination detection in large video-language models (LVLMs). VideoHallucer categorizes hallucinations into two main types: intrinsic and extrinsic, offering further subcategories for detailed analysis, including object-relation, temporal, semantic detail, extrinsic factual, and extrinsic non-factual hallucinations. We adopt an adversarial binary VideoQA method for comprehensive evaluation, where pairs of basic and hallucinated questions are crafted strategically. By evaluating eleven LVLMs on VideoHallucer, we reveal that i) the majority of current models exhibit significant issues with hallucinations; ii) while scaling datasets and parameters improves models' ability to detect basic visual cues and counterfactuals, it provides limited benefit for detecting extrinsic factual hallucinations; iii) existing models are more adept at detecting facts than identifying hallucinations. As a byproduct, these analyses further instruct the development of our self-PEP framework, achieving an average of 5.38% improvement in hallucination resistance across all model architectures.",
          "https://arxiv.org/abs/2406.16338",
          "https://videohallucer.github.io/"
          )



        add_paper("Autoregressive Pretraining with Mamba in Vision",
          "Sucheng Ren, Xianhang Li, Haoqin Tu, Feng Wang, Fangxun Shu, Lei Zhang, Jieru Mei, Linjie Yang, Peng Wang, Heng Wang, Alan Yuille, Cihang Xie",
          null,
          "https://arxiv.org/abs/2406.07537",
          "@article{ren2024autoregressive,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {Autoregressive Pretraining with Mamba in Vision},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Ren, Sucheng and Li, Xianhang and Tu, Haoqin and Wang, Feng and Shu, Fangxun and Zhang, Lei and Mei, Jieru and Yang, Linjie and Wang, Peng and Wang, Heng and Yuille, Alan and Xie, Cihang},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2406.07537},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "The vision community has started to build with the recently developed state space model, Mamba, as the new backbone for a range of tasks. This paper shows that Mamba's visual capability can be significantly enhanced through autoregressive pretraining, a direction not previously explored. Efficiency-wise, the autoregressive nature can well capitalize on the Mamba's unidirectional recurrent structure, enabling faster overall training speed compared to other training strategies like mask modeling. Performance-wise, autoregressive pretraining equips the Mamba architecture with markedly higher accuracy over its supervised-trained counterparts and, more importantly, successfully unlocks its scaling potential to large and even huge model sizes. For example, with autoregressive pretraining, a base-size Mamba attains 83.2% ImageNet accuracy, outperforming its supervised counterpart by 2.0%; our huge-size Mamba, the largest Vision Mamba to date, attains 85.0% ImageNet accuracy (85.5% when finetuned with 384x384 inputs), notably surpassing all other Mamba variants in vision. The code is available at https://github.com/OliverRensu/ARM.",
          "https://arxiv.org/abs/2406.07537",
          "https://github.com/OliverRensu/ARM"
        )




        add_paper("ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning",
          "Sucheng Ren, Hongru Zhu, Chen Wei, Yijiang Li, Alan Yuille, Cihang Xie",
          null,
          "https://arxiv.org/abs/2405.15160",
          "@article{ren2024arvideo,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Ren, Sucheng and Zhu, Hongru and Wei, Chen and Li, Yijiang and Yuille, Alan and Xie, Cihang},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2405.15160},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "This paper presents a new self-supervised video representation learning framework, ARVideo, which autoregressively predicts the next video token in a tailored sequence order. Two key designs are included. First, we organize autoregressive video tokens into clusters that span both spatially and temporally, thereby enabling a richer aggregation of contextual information compared to the standard spatial-only or temporal-only clusters. Second, we adopt a randomized spatiotemporal prediction order to facilitate learning from multi-dimensional data, addressing the limitations of a handcrafted spatial-first or temporal-first sequence order. Extensive experiments establish ARVideo as an effective paradigm for self-supervised video representation learning. For example, when trained with the ViT-B backbone, ARVideo competitively attains 81.2% on Kinetics-400 and 70.9% on Something-Something V2, which are on par with the strong benchmark set by VideoMAE. Importantly, ARVideo also demonstrates higher training efficiency, i.e., it trains 14% faster and requires 58% less GPU memory compared to VideoMAE.",
          "https://arxiv.org/abs/2405.15160",
          null
        )


        add_paper("Mamba-R: Vision Mamba ALSO Needs Registers",
          "Feng Wang, Jiahao Wang, Sucheng Ren, Guoyizhe Wei, Jieru Mei, Wei Shao, Yuyin Zhou, Alan Yuille, Cihang Xie",
          null,
          "https://arxiv.org/abs/2405.14858",
          "@article{wang2024mambar,<br>" +
           "&nbsp;&nbsp;&nbsp;title     = {Mamba-R: Vision Mamba also needs registers},<br>" +
           "&nbsp;&nbsp;&nbsp;author    = {Wang, Feng and Wang, Jiahao and Ren, Sucheng and Wei, Guoyizhe and Mei, Jieru and Shao, Wei and Zhou, Yuyin and Yuille, Alan and Xie, Cihang},<br>" +
           "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2405.14858},<br>" +
           "&nbsp;&nbsp;&nbsp;year      = {2024},<br>",
          "Similar to Vision Transformers, this paper identifies artifacts also present within the feature maps of Vision Mamba. These artifacts, corresponding to high-norm tokens emerging in low-information background areas of images, appear much more severe in Vision Mamba -- they exist prevalently even with the tiny-sized model and activate extensively across background regions. To mitigate this issue, we follow the prior solution of introducing register tokens into Vision Mamba. To better cope with Mamba blocks' uni-directional inference paradigm, two key modifications are introduced: 1) evenly inserting registers throughout the input token sequence, and 2) recycling registers for final decision predictions. We term this new architecture Mamba-R. Qualitative observations suggest, compared to vanilla Vision Mamba, Mamba-R's feature maps appear cleaner and more focused on semantically meaningful regions. Quantitatively, Mamba-R attains stronger performance and scales better. For example, on the ImageNet benchmark, our base-size Mamba-R attains 82.9% accuracy, significantly outperforming Vim-B's 81.8%; furthermore, we provide the first successful scaling to the large model size (i.e., with 341M parameters), attaining a competitive accuracy of 83.2% (84.5% if finetuned with 384x384 inputs). Additional validation on the downstream semantic segmentation task also supports Mamba-R's efficacy.",
          "https://arxiv.org/abs/2405.14858",
          "https://wangf3014.github.io/mambar-page/"
        )

        add_paper("A Flexible 2.5D Medical Image Segmentation Approach with In-Slice and Cross-Slice Attention",
          "Amarjeet Kumar, Hongxu Jiang, Muhammad Imran, Cyndi Valdes, Gabriela Leon, Dahyun Kang, Parvathi Nataraj, Yuyin Zhou, Michael D Weiss, Wei Shao",
          null,
          "https://arxiv.org/abs/2405.00130",
          "@article{kumar2024flexible25d,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {A Flexible 2.5 D Medical Image Segmentation Approach with In-Slice and Cross-Slice Attention},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Kumar, Amarjeet and Jiang, Hongxu and Imran, Muhammad and Valdes, Cyndi and Leon, Gabriela and Kang, Dahyun and Nataraj, Parvathi and Zhou, Yuyin and Weiss, Michael D and Shao, Wei},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2405.00130},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "Deep learning has become the de facto method for medical image segmentation, with 3D segmentation models excelling in capturing complex 3D structures and 2D models offering high computational efficiency. However, segmenting 2.5D images, which have high in-plane but low through-plane resolution, is a relatively unexplored challenge. While applying 2D models to individual slices of a 2.5D image is feasible, it fails to capture the spatial relationships between slices. On the other hand, 3D models face challenges such as resolution inconsistencies in 2.5D images, along with computational complexity and susceptibility to overfitting when trained with limited data. In this context, 2.5D models, which capture inter-slice correlations using only 2D neural networks, emerge as a promising solution due to their reduced computational demand and simplicity in implementation. In this paper, we introduce CSA-Net, a flexible 2.5D segmentation model capable of processing 2.5D images with an arbitrary number of slices through an innovative Cross-Slice Attention (CSA) module. This module uses the cross-slice attention mechanism to effectively capture 3D spatial information by learning long-range dependencies between the center slice (for segmentation) and its neighboring slices. Moreover, CSA-Net utilizes the self-attention mechanism to understand correlations among pixels within the center slice. We evaluated CSA-Net on three 2.5D segmentation tasks: (1) multi-class brain MRI segmentation, (2) binary prostate MRI segmentation, and (3) multi-class prostate MRI segmentation. CSA-Net outperformed leading 2D and 2.5D segmentation methods across all three tasks, demonstrating its efficacy and superiority. Our code is publicly available at https://github.com/mirthAI/CSA-Net.",
          "https://arxiv.org/abs/2405.00130",
          "https://github.com/mirthAI/CSA-Net"
        )


        add_paper("RetinaRegNet: A Versatile Approach for Retinal Image Registration",
          "Vishal Balaji Sivaraman, Muhammad Imran, Qingyue Wei, Preethika Muralidharan, Michelle R Tamplin, Isabella M Grumbach, Randy H Kardon, Jui-Kai Wang, Yuyin Zhou, Wei Shao",
          null,
          "https://arxiv.org/abs/2404.16017",
          "@article{sivaraman2024retinaregnet,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {RetinaRegNet: A Versatile Approach for Retinal Image Registration},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Sivaraman, Vishal Balaji and Imran, Muhammad and Wei, Qingyue and Muralidharan, Preethika and Tamplin, Michelle R and Grumbach, Isabella M and Kardon, Randy H and Wang, Jui-Kai and Zhou, Yuyin and Shao, Wei},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2404.16017},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "We introduce the RetinaRegNet model, which can achieve state-of-the-art performance across various retinal image registration tasks. RetinaRegNet does not require training on any retinal images. It begins by establishing point correspondences between two retinal images using image features derived from diffusion models. This process involves the selection of feature points from the moving image using the SIFT algorithm alongside random point sampling. For each selected feature point, a 2D correlation map is computed by assessing the similarity between the feature vector at that point and the feature vectors of all pixels in the fixed image. The pixel with the highest similarity score in the correlation map corresponds to the feature point in the moving image. To remove outliers in the estimated point correspondences, we first applied an inverse consistency constraint, followed by a transformation-based outlier detector. This method proved to outperform the widely used random sample consensus (RANSAC) outlier detector by a significant margin. To handle large deformations, we utilized a two-stage image registration framework. A homography transformation was used in the first stage and a more accurate third-order polynomial transformation was used in the second stage. The model's effectiveness was demonstrated across three retinal image datasets: color fundus images, fluorescein angiography images, and laser speckle flowgraphy images. RetinaRegNet outperformed current state-of-the-art methods in all three datasets. It was especially effective for registering image pairs with large displacement and scaling deformations. This innovation holds promise for various applications in retinal image analysis. Our code is publicly available at https://github.com/mirthAI/RetinaRegNet.",
          "https://arxiv.org/abs/2404.16017",
          "https://github.com/mirthAI/RetinaRegNet"
        )


        add_paper("HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing",
          "Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Cihang Xie, Yuyin Zhou",
          null,
          "https://arxiv.org/abs/2404.09990",
          "@article{hui2024hqedit,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Hui, Mude and Yang, Siwei and Zhao, Bingchen and Shi, Yichun and Wang, Heng and Wang, Peng and Xie, Cihang and Zhou, Yuyin},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2404.09990},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "This study introduces HQ-Edit, a high-quality instruction-based image editing dataset with around 200,000 edits. Unlike prior approaches relying on attribute guidance or human feedback on building datasets, we devise a scalable data collection pipeline leveraging advanced foundation models, namely GPT-4V and DALL-E 3. To ensure its high quality, diverse examples are first collected online, expanded, and then used to create high-quality diptychs featuring input and output images with detailed text prompts, followed by precise alignment ensured through post-processing. In addition, we propose two evaluation metrics, Alignment and Coherence, to quantitatively assess the quality of image edit pairs using GPT-4V. HQ-Edits high-resolution images, rich in detail and accompanied by comprehensive editing prompts, substantially enhance the capabilities of existing image editing models. For example, an HQ-Edit finetuned InstructPix2Pix can attain state-of-the-art image editing performance, even surpassing those models fine-tuned with human-annotated data. The project page is https://thefllood.github.io/HQEdit_web/",
          "https://arxiv.org/abs/2404.09990",
          "https://thefllood.github.io/HQEdit_web/"
        )


        add_paper("AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability",
          "Siwei Yang, Bingchen Zhao, Cihang Xie",
          null,
          "https://arxiv.org/abs/2402.09404",
          "@article{yang2024aqabench,<br>" +
           "&nbsp;&nbsp;&nbsp;title   = {AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability},<br>" +
           "&nbsp;&nbsp;&nbsp;author  = {Yang, Siwei and Zhao, Bingchen and Xie, Cihang},<br>" +
           "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2402.09404},<br>" +
           "&nbsp;&nbsp;&nbsp;year    = {2024},<br>",
          "This paper introduces AQA-Bench, a novel benchmark to assess the sequential reasoning capabilities of large language models (LLMs) in algorithmic contexts, such as depth-first search (DFS). The key feature of our evaluation benchmark lies in its interactive evaluation protocol -- for example, in DFS, the availability of each node's connected edge is contingent upon the model's traversal to that node, thereby necessitating the LLM's ability to effectively remember visited nodes and strategize subsequent moves. We comprehensively build AQA-Bench with three different algorithms, namely binary search, depth-first search, and breadth-first search, and to evaluate the sequential reasoning ability of 12 different LLMs. Our investigations reveal several interesting findings: (1) Closed-source models like GPT-4 and Gemini generally show strong sequential reasoning ability, significantly outperforming open-source LLMs. (2) Naively providing interactive examples may inadvertently hurt few-shot performance. (3) A very limited number of predecessor steps following the optimal policy can substantially boost small models' performance. (4) The scaling correlation between performance and model size is not always significant, sometimes even showcasing an inverse trend. We hope our study can catalyze future work on advancing the understanding and enhancement of LLMs' capabilities in sequential reasoning. The code is available at https://github.com/UCSC-VLAA/AQA-Bench",
          "https://arxiv.org/abs/2402.09404",
          "https://github.com/UCSC-VLAA/AQA-Bench"
        )





        add_paper("SPFormer: Enhancing Vision Transformer with Superpixel Representation",
          "Jieru Mei, Liang-Chieh Chen, Alan Yuille, Cihang Xie",
          null,
          "https://arxiv.org/abs/2401.02931",
          "@article{mei2024spformer,<br>" +
          "&nbsp;&nbsp;&nbsp;title     = {SPFormer: Enhancing Vision Transformer with Superpixel Representation},<br>" +
          "&nbsp;&nbsp;&nbsp;author    = {Mei, Jieru and Chen, Liang-Chieh and Yuille, Alan and Xie, Cihang},<br>" +
          "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2401.02931},<br>" +
          "&nbsp;&nbsp;&nbsp;year      = {2024}<br>}",
          "In this work, we introduce SPFormer, a novel Vision Transformer enhanced by superpixel representation. Addressing the limitations of traditional Vision Transformers' fixed-size, non-adaptive patch partitioning, SPFormer employs superpixels that adapt to the image's content. This approach divides the image into irregular, semantically coherent regions, effectively capturing intricate details and applicable at both initial and intermediate feature levels. SPFormer, trainable end-to-end, exhibits superior performance across various benchmarks. Notably, it exhibits significant improvements on the challenging ImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S respectively. A standout feature of SPFormer is its inherent explainability. The superpixel structure offers a window into the model's internal processes, providing valuable insights that enhance the model's interpretability. This level of clarity significantly improves SPFormer's robustness, particularly in challenging scenarios such as image rotations and occlusions, demonstrating its adaptability and resilience.",
          "https://arxiv.org/abs/2401.02931"
        )


        add_paper("Audio-Visual LLM for Video Understanding",
                  "Hangxun Shu, Lei Zhang, Hao Jiang, Cihang Xie",
                  null,
                  "https://arxiv.org/abs/2312.06720",
                  "@article{shu2023audio,<br>" +
                  "&nbsp;&nbsp;&nbsp;title     = {Audio-Visual LLM for Video Understanding},<br>" +
                  "&nbsp;&nbsp;&nbsp;author    = {Shu, Fangxun and Zhang, Lei and Jiang, Hao and Xie, Cihang},<br>" +
                  "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2312.06720},<br>" +
                  "&nbsp;&nbsp;&nbsp;year      = {2023}<br>}",
                  "This paper presents Audio-Visual LLM, a Multimodal Large Language Model that takes both visual and auditory inputs for holistic video understanding. A key design is the modality-augmented training, which involves the integration of modality-specific tokens engineered to activate the appropriate visual and/or auditory encoder selectively. This mechanism is pivotal in enabling end-to-end joint training with video data at different modalities, including visual-only, audio-only, and audio-visual formats. Moreover, we introduce a high-quality video instruction dataset, derived from GPT-4. This dataset allows Audio-Visual LLM to adeptly process a variety of task-oriented video instructions, ranging from multi-turn conversations and audio-visual narratives to complex reasoning tasks. Extensive experiments demonstrate that Audio-Visual LLM impressively achieves strong zero-shot results across a range of video understanding tasks. For example, Audio-Visual LLM achieves an accuracy of 53.7% on MSRVTT-QA, outperforming non-LLM-based InterVideo by 6.6% and LLM-based Valley by 4.4%, respectively. Additionally, our Audio-Visual LLM also achieves competitive performance on audio tasks (e.g., AudioCaps).",
                  "https://arxiv.org/abs/2312.06720"
                  )


        add_paper("Compress & Align: Curating Image-Text Data with Human Knowledge",
                  "Lei Zhang, Fangxun Shu, Sucheng Ren, Hao Jiang, Bingchen Zhao, Cihang Xie",
                  null,
                  "https://arxiv.org/abs/2312.06726",
                  "@article{zhang2023compress,<br>" +
                  "&nbsp;&nbsp;&nbsp;title     = {Compress & Align: Curating Image-Text Data with Human Knowledge},<br>" +
                  "&nbsp;&nbsp;&nbsp;author    = {Zhang, Lei and Shu, Fangxun and Ren, Sucheng and Zhao, Bingchen and Jiang, Hao and Xie, Cihang},<br>" +
                  "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2312.06726},<br>" +
                  "&nbsp;&nbsp;&nbsp;year      = {2023}<br>}",
                  "The massive growth of image-text data through web crawling inherently presents the challenge of variability in data quality. This paper introduces a novel algorithm, rooted in human knowledge, to compress this vast corpus of web-crawled image-text datasets to a compact and high-quality form. Our method unfolds in three major steps. First, we collect an image-text dataset, wherein each image is associated with multiple captions sourced from diverse origins. Then, to systemically capture human preferences regarding the best caption paired with each image, we establish a comprehensive set of both subjective and objective criteria for critically guiding the alignment assessment from labelers. Lastly, we train a reward model on the annotated dataset to internalize the nuanced human understanding of image-text alignment. The resulting reward model thus can act as a human-like referee to filter misaligned/low-quality image-text pairs. Extensive experiments demonstrate that we are able to secure (or even improve) model performance by compressing the image-text datasets up to ~90%. An impressive example is that, by aggressively reducing the total training sample from 130M to 15.5M (e.g., ~9x smaller), our BLIP-B/16 models still consistently show superior performance compared with the full-size-dataset counterpart on image-text retrieval (Flickr30K, COCO) by ~2.5% in Recall@1, and on image-captioning (Nocaps, COCO) by ~10.0% in CIDEr and ~2.7% in SPICE.",
                  "https://arxiv.org/abs/2312.06726"
                  )




        // add_paper("3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers",
        //           "Jieneng Chen, Jieru Mei, Xianhang Li, Yongyi Lu, Qihang Yu, Qingyue Wei, Xiangde Luo, Yutong Xie, Ehsan Adeli, Yan Wang, Matthew Lungren, Lei Xing, Le Lu, Alan Yuille, Yuyin Zhou",
        //           null,
        //           "https://arxiv.org/abs/2310.07781",
        //           "@article{chen2023transunet,<br>" +
        //           "&nbsp;&nbsp;&nbsp;title     = {3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers},<br>" +
        //           "&nbsp;&nbsp;&nbsp;author    = {Chen, Jieneng and Mei, Jieru and Li, Xianhang and Lu, Yongyi and Yu, Qihang and Wei, Qingyue and Luo, Xiangde and Xie, Yutong and Adeli, Ehsan and Wang, Yan and Lungren, Matthew and Xing, Lei and Lu, Le and Yuille, Alan and Zhou, Yuyin},<br>" +
        //           "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2310.07781},<br>" +
        //           "&nbsp;&nbsp;&nbsp;year      = {2023}<br>}",
        //           "Medical image segmentation plays a crucial role in advancing healthcare systems for disease diagnosis and treatment planning. The u-shaped architecture, popularly known as U-Net, has proven highly successful for various medical image segmentation tasks. However, U-Net's convolution-based operations inherently limit its ability to model long-range dependencies effectively. To address these limitations, researchers have turned to Transformers, renowned for their global self-attention mechanisms, as alternative architectures. Our previous TransUNet, which leverages Transformers' self-attention to complement U-Net's localized information with the global context, is now extended to a 3D network. This is achieved by building upon the state-of-the-art nnU-Net architecture, fully exploring Transformers' potential in both the encoder and decoder design. We introduce a Transformer encoder for tokenizing image patches and a Transformer decoder for adaptively refining candidate regions. The Transformer encoder excels in multi-organ segmentation, while the Transformer decoder is more beneficial for small and challenging segmented targets such as tumor segmentation. Our extensive experiments showcase the significant potential of integrating a Transformer-based encoder and decoder into the u-shaped medical image segmentation architecture, with TransUNet outperforming competitors in various medical applications.",
        //           "https://arxiv.org/abs/2310.07781",
        //           "https://github.com/Beckschen/3D-TransUNet"
        //           )



 


        // add_paper("BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks",
        //             "Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, Lifang He, Brian D Davison, Quanzheng Li, Yong Chen, Hongfang Liu, Lichao Sun",
        //             null,
        //             "https://arxiv.org/abs/2305.17100",
        //             "@article{zhang2023biomedgpt,<br>" +
        //             "&nbsp;&nbsp;&nbsp;title = {BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks},<br>" +
        //             "&nbsp;&nbsp;&nbsp;author = {Zhang, Kai and Yu, Jun and Yan, Zhiling and Liu, Yixin and Adhikarla, Eashan and Fu, Sunyang and Chen, Xun and Chen, Chen and Zhou, Yuyin and Li, Xiang and He, Lifang and Davison, Brian D and Li, Quanzheng and Chen, Yong and Liu, Hongfang and Sun, Lichao},<br>" +
        //             "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2305.17100}<br>" +
        //             "&nbsp;&nbsp;&nbsp;year = {2023},<br>",
        //             "In this paper, we introduce a unified and generalist Biomedical Generative Pre-trained Transformer (BiomedGPT) model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks. Our experiments demonstrate that BiomedGPT delivers expansive and inclusive representations of biomedical data, outperforming the majority of preceding state-of-the-art models across five distinct tasks with 20 public datasets spanning over 15 unique biomedical modalities. Through the ablation study, we also showcase the efficacy of our multi-modal and multi-task pretraining approach in transferring knowledge to previously unseen data. Overall, our work presents a significant step forward in developing unified and generalist models for biomedicine, with far-reaching implications for improving healthcare outcomes.",
        //             "https://arxiv.org/abs/2305.17100",
        //             )
        
        add_paper("Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation",
                    "Haipeng Zhou, Lei Zhu, Yuyin Zhou",
                    // "arxiv, 2023",
                    null,
                    "https://arxiv.org/abs/2303.12313",
                    "@article{zhou2023distribution,<br>" +
                    "&nbsp;&nbsp;&nbsp;title={Distribution Aligned Diffusion and Prototype-guided network for Unsupervised Domain Adaptive Segmentation},<br>" +
                    "&nbsp;&nbsp;&nbsp;author={Zhou, Haipeng and Zhu, Lei and Zhou, Yuyin},<br>" +
                    "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2303.12313},<br>" +
                    "&nbsp;&nbsp;&nbsp;year={2023}<br>}",
                    "The Diffusion Probabilistic Model (DPM) has emerged as a highly effective generative model in the field of computer vision. Its intermediate latent vectors offer rich semantic information, making it an attractive option for various downstream tasks such as segmentation and detection. In order to explore its potential further, we have taken a step forward and considered a more complex scenario in the medical image domain, specifically, under an unsupervised adaptation condition. To this end, we propose a Diffusion-based and Prototype-guided network (DP-Net) for unsupervised domain adaptive segmentation. Concretely, our DP-Net consists of two stages: 1) Distribution Aligned Diffusion (DADiff), which involves training a domain discriminator to minimize the difference between the intermediate features generated by the DPM, thereby aligning the inter-domain distribution; and 2) Prototype-guided Consistency Learning (PCL), which utilizes feature centroids as prototypes and applies a prototype-guided loss to ensure that the segmentor learns consistent content from both source and target domains. Our approach is evaluated on fundus datasets through a series of experiments, which demonstrate that the performance of the proposed method is reliable and outperforms state-of-the-art methods. Our work presents a promising direction for using DPM in complex medical image scenarios, opening up new possibilities for further research in medical imaging.",
                    "https://arxiv.org/abs/2303.12313"
                    )        



        add_paper("Bag of Tricks for FGSM Adversarial Training",
                    "Zichao Li, Li Liu, Zeyu Wang, Yuyin Zhou, Cihang Xie",
                    null,
                    "https://arxiv.org/abs/2209.02684",
                    "@article{li2022bag,<br>" +
                     "&nbsp;&nbsp;&nbsp;title   = {Bag of Tricks for FGSM Adversarial Training},<br>" +
                     "&nbsp;&nbsp;&nbsp;author  = {Li, Zichao and Liu, Li and Wang, Zeyu and Zhou, Yuyin and Xie, Cihang},<br>" +
                     "&nbsp;&nbsp;&nbsp;journal = {arXiv preprint arXiv:2209.02684},<br>" +
                     "&nbsp;&nbsp;&nbsp;year   = {2022}<br>}",
                    "Adversarial training (AT) with samples generated by Fast Gradient Sign Method (FGSM), also known as FGSM-AT, is a computationally simple method to train robust networks. However, during its training procedure, an unstable mode of “catastrophic overfitting” has been identified in [Wong et al., 2020], where the robust accuracy abruptly drops to zero within a single training step. Existing methods use gradient regularizers or random initialization tricks to attenuate this issue, whereas they either take high computational cost or lead to lower robust accuracy. In this work, we provide the first study, which thoroughly examines a collection of tricks from three perspectives: Data Initialization, Network Structure, and Optimization, to overcome the catastrophic overfitting in FGSM-AT. Surprisingly, we find that simple tricks, i.e., a) masking partial pixels (even without randomness), b) setting a large convolution stride and smooth activation functions, or c) regularizing the weights of the first convolutional layer, can effectively tackle the overfitting issue. Extensive results on a range of network architectures validate the effectiveness of each proposed trick, and the combinations of tricks are also investigated. For example, trained with PreActResNet-18 on CIFAR-10, our method attains 49.8% accuracy against PGD-50 attacker and 46.4% accuracy against AutoAttack, demonstrating that pure FGSM-AT is capable of enabling robust learners. The code and models are publicly available at https://github. com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT.",
                    "https://arxiv.org/abs/2209.02684",
                    "https://github. com/UCSC-VLAA/Bag-of-Tricks-for-FGSM-AT"
                )

         



        add_paper("The FELIX Project: Deep Networks To Detect Pancreatic Neoplasms",
                    "Yingda Xia, Qihang Yu, Linda Chu, Satomi Kawamoto, Seyoun Park, Fengze Liu, Jieneng Chen, Zhuotun Zhu, Bowen Li, Zongwei Zhou, Yongyi Lu, Yan Wang, Wei Shen, Lingxi Xie, Yuyin Zhou, Christopher Wolfgang, Ammar Javed, Daniel Fadaei Fouladi, Shahab Shayesteh, Jefferson Graves, Alejandra Blanco, Eva S Zinreich, Benedict Kinny-Köster, Kenneth Kinzler, Ralph H Hruban, Bert Vogelstein, Alan Yuille, Elliot K Fishman",
                    null,
                    "https://www.medrxiv.org/content/10.1101/2022.09.24.22280071v1",
                    "@article{xia2022felix,<br>" +
                     "&nbsp;&nbsp;&nbsp;title     = {The FELIX Project: Deep Networks To Detect Pancreatic Neoplasms},<br>" +
                     "&nbsp;&nbsp;&nbsp;author    = {Xia, Yingda and Yu, Qihang and Chu, Linda and Kawamoto, Satomi and Park, Seyoun and Liu, Fengze and Chen, Jieneng and Zhu, Zhuotun and Li, Bowen and Zhou, Zongwei and others},<br>" +
                     "&nbsp;&nbsp;&nbsp;journal   = {medRxiv},<br>" +
                     "&nbsp;&nbsp;&nbsp;year      = {2022},<br>",
                    "Tens of millions of abdominal images are performed with computed tomography (CT) in the U.S. each year but pancreatic cancers are sometimes not initially detected in these images. We here describe a suite of algorithms (named FELIX) that can recognize pancreatic lesions from CT images without human input. Using FELIX, >90% of patients with pancreatic ductal adenocarcinomas were detected at a specificity of >90% in patients without pancreatic disease. FELIX may be able to assist radiologists in identifying pancreatic cancers earlier, when surgery and other treatments offer more hope for long-term survival.",
                    "https://www.medrxiv.org/content/10.1101/2022.09.24.22280071v1"
                )





        // 2021 preprint

        add_paper("Radfusion: Benchmarking performance and fairness for multimodal pulmonary embolism detection from ct and ehr",
            "Yuyin Zhou, Shih-Cheng Huang, Jason Alan Fries, Alaa Youssef, Timothy J Amrhein, Marcello Chang, Imon Banerjee, Daniel Rubin, Lei Xing, Nigam Shah, Matthew P Lungren",
                    null,
            "https://arxiv.org/abs/2111.11665",
            "@article{zhou2021radfusion,<br>" +
              "&nbsp;&nbsp;&nbsp;title     = {Radfusion: Benchmarking performance and fairness for multimodal pulmonary embolism detection from ct and ehr},<br>" +
              "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Huang, Shih-Cheng and Fries, Jason Alan and Youssef, Alaa and Amrhein, Timothy J and Chang, Marcello and Banerjee, Imon and Rubin, Daniel and Xing, Lei and Shah, Nigam and others},<br>" +
              "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2111.11665},<br>" +
              "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
            "Despite the routine use of electronic health record (EHR) data by radiologists to contextualize clinical history and inform image interpretation, the majority of deep learning architectures for medical imaging are unimodal, i.e., they only learn features from pixel-level information. Recent research revealing how race can be recovered from pixel data alone highlights the potential for serious biases in models which fail to account for demographics and other key patient attributes. Yet the lack of imaging datasets which capture clinical context, inclusive of demographics and longitudinal medical history, has left multimodal medical imaging underexplored. To better assess these challenges, we present RadFusion, a multimodal, benchmark dataset of 1794 patients with corresponding EHR data and high-resolution computed tomography (CT) scans labeled for pulmonary embolism. We evaluate several representative multimodal fusion models and benchmark their fairness properties across protected subgroups, e.g., gender, race/ethnicity, age. Our results suggest that integrating imaging and EHR data can improve classification performance and robustness without introducing large disparities in the true positive rate between population groups.",
            "https://arxiv.org/abs/2111.11665"
        )


        // add_paper("Transunet: Transformers make strong encoders for medical image segmentation",
        //             "Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan Yuille, Yuyin Zhou",
        //             // "arXiv, 2021",
        //             null,
        //             "https://arxiv.org/abs/2102.04306",
        //             "@article{chen2021transunet,<br>" +
        //              "&nbsp;&nbsp;&nbsp;title     = {Transunet: Transformers make strong encoders for medical image segmentation},<br>" +
        //              "&nbsp;&nbsp;&nbsp;author    = {Chen, Jieneng and Lu, Yongyi and Yu, Qihang and Luo, Xiangde and Adeli, Ehsan and Wang, Yan and Lu, Le and Yuille, Alan and Zhou, Yuyin},<br>" +
        //              "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2102.04306},<br>" +
        //              "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
        //             "Medical image segmentation is an essential prerequisite for developing healthcare systems, especially for disease diagnosis and treatment planning. On various medical image segmentation tasks, the u-shaped architecture, also known as U-Net, has become the de-facto standard and achieved tremendous success. However, due to the intrinsic locality of convolution operations, U-Net generally demonstrates limitations in explicitly modeling long-range dependency. Transformers, designed for sequence-to-sequence prediction, have emerged as alternative architectures with innate global self-attention mechanisms, but can result in limited localization abilities due to insufficient low-level details. In this paper, we propose TransUNet, which merits both Transformers and U-Net, as a strong alternative for medical image segmentation. On one hand, the Transformer encodes tokenized image patches from a convolution neural network (CNN) feature map as the input sequence for extracting global contexts. On the other hand, the decoder upsamples the encoded features which are then combined with the high-resolution CNN feature maps to enable precise localization. We argue that Transformers can serve as strong encoders for medical image segmentation tasks, with the combination of U-Net to enhance finer details by recovering localized spatial information. TransUNet achieves superior performances to various competing methods on different medical applications including multi-organ segmentation and cardiac segmentation. Code and models are available at https://github.com/Beckschen/TransUNet.",
        //             "https://arxiv.org/abs/2102.04306",
        //             "https://github.com/Beckschen/TransUNet"
        // )


        add_paper("Can temporal information help with contrastive self-supervised learning?",
                    "Yutong Bai, Haoqi Fan, Ishan Misra, Ganesh Venkatesh, Yongyi Lu, Yuyin Zhou, Qihang Yu, Vikas Chandra, Alan Yuille",
                    null,
                    "https://arxiv.org/abs/2011.13046",
                    "@article{bai2020can,<br>" +
                     "&nbsp;&nbsp;&nbsp;title     = {Can temporal information help with contrastive self-supervised learning?},<br>" +
                     "&nbsp;&nbsp;&nbsp;author    = {Bai, Yutong and Fan, Haoqi and Misra, Ishan and Venkatesh, Ganesh and Lu, Yongyi and Zhou, Yuyin and Yu, Qihang and Chandra, Vikas and Yuille, Alan},<br>" +
                     "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2011.13046},<br>" +
                     "&nbsp;&nbsp;&nbsp;year      = {2020}<br>" ,
                    "Leveraging temporal information has been regarded as essential for developing video understanding models. However, how to properly incorporate temporal information into the recent successful instance discrimination based contrastive self-supervised learning (CSL) framework remains unclear. As an intuitive solution, we find that directly applying temporal augmentations does not help, or even impair video CSL in general. This counter-intuitive observation motivates us to re-design existing video CSL frameworks, for better integration of temporal knowledge. To this end, we present Temporal-aware Contrastive self-supervised learningTaCo, as a general paradigm to enhance video CSL. Specifically, TaCo selects a set of temporal transformations not only as strong data augmentation but also to constitute extra self-supervision for video understanding. By jointly contrasting instances with enriched temporal transformations and learning these transformations as self-supervised signals, TaCo can significantly enhance unsupervised video representation learning. For instance, TaCo demonstrates consistent improvement in downstream classification tasks over a list of backbones and CSL approaches. Our best model achieves 85.1% (UCF-101) and 51.6% (HMDB-51) top-1 accuracy, which is a 3% and 2.4% relative improvement over the previous state-of-the-art.",
                    "https://arxiv.org/abs/2011.13046"
        )


         add_paper("Smooth Adversarial Training",
                    "Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, Quoc Le",
                    null,
                    "https://arxiv.org/abs/2006.14536",
                    "@article{xie2020smooth,<br>" +
                     "&nbsp;&nbsp;&nbsp;title     = {Smooth adversarial training},<br>" +
                     "&nbsp;&nbsp;&nbsp;author    = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Yuille, Alan and Le, Quoc V},<br>" +
                     "&nbsp;&nbsp;&nbsp;journal   = {arXiv preprint arXiv:2006.14536},<br>" +
                     "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
                    "It is commonly believed that networks cannot be both accurate and robust, that gaining robustness means losing accuracy. It is also generally believed that, unless making networks larger, network architectural elements would otherwise matter little in improving adversarial robustness. Here we present evidence to challenge these common beliefs by a careful study about adversarial training. Our key observation is that the widely-used ReLU activation function significantly weakens adversarial training due to its non-smooth nature. Hence we propose smooth adversarial training (SAT), in which we replace ReLU with its smooth approximations to strengthen adversarial training. The purpose of smooth activation functions in SAT is to allow it to f ind harder adversarial examples and compute better gradient updates during adversarial training. Compared to standard adversarial training, SAT improves adversarial robustness for “free”, i.e., no drop in accuracy and no increase in computational cost. For example, without introducing additional computations, SAT significantly enhances ResNet-50’s robustness from 33.0% to 42.3%, while also improving accuracy by 0.9% on ImageNet. SAT also works well with larger networks: it helps EfficientNet-L1 to achieve 82.2% accuracy and 58.6% robustness on ImageNet, outperforming the previous state-ofthe-art defense by 9.5% for accuracy and 11.6% for robustness. Models are available at https://github.com/ cihangxie/SmoothAdversarialTraining.",
                    "https://arxiv.org/abs/2006.14536",
                    "https://github.com/cihangxie/SmoothAdversarialTraining"
                )


</script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
</details>




<!-- --------------------------------------Publications -------------------->


<script>
        paper_count = paper_count

        function add_paper(title, authors, conference, link, bib, abstract, arxiv_link, code, press, slides, talk, msg) {
            list_entry = "<li style=\"font-size:18px\">"
            if (link != null)
                list_entry += "<a href=\"" + link + "\">"
            list_entry += "<b>" + title + "</b>"
            if (link != null)
                list_entry += "</a>"
            list_entry += "<br>" + authors + ".<br>" 
            if (conference != null)
                list_entry+= "<i><font color=\" #707070\">" + conference + "</font></i>.</li>"
            if (bib != null) {
                list_entry += "<div id=\"bib" + paper_count + "\" style=\"display:none\">" + bib + "</div>"
                list_entry += "<a href=\"javascript:copy(div" + paper_count + ",bib" + paper_count + ")\"> <span class=\"label label-success\">bib</span></a>"
            }

            if (abstract != null) {
                list_entry += "<div id=\"abstract" + paper_count + "\" style=\"display:none\">" + abstract + "</div>"
                list_entry += "<a href=\"javascript:copy(div" + paper_count + ",abstract" + paper_count + ")\"> <span class=\"label label-warning\">abstract</span></a>"
            }
            if (arxiv_link != null)
                list_entry += " <a href=\"" + arxiv_link + "\"><span class=\"label label-primary\">arxiv</span></a>"

            if (code != null)
                list_entry += " <a href=\"" + code + "\"><span class=\"label label-danger\">code/models</span></a>"

            if (press != null)
                list_entry += " <a href=\"" + press + "\"><span class=\"label label-success\">press</span></a>"

            if (slides != null)
                list_entry += " <a href=\"" + slides + "\"><span class=\"label label-info\">slides/poster</span></a>"

            if (talk != null)
                list_entry += " <a href=\"" + talk + "\"><span class=\"label label-default\">talk</span></a>"

            list_entry += "<br>"

            if (msg != null)
                list_entry += "<i>" + msg + "</i>"

            list_entry += "<div id=\"div" + paper_count + "\" style=\"font-size:15px\"></div><br>"

            document.write(list_entry)

            paper_count += 1
        }

        document.write("</ul>")
        document.write("<ul>")


        document.write("</ul><br>")
        document.write("<h1>2024</h1>")
        document.write("<ul>")

        add_paper("Hydrodynamics in Semidilute Polyelectrolyte Solutions and Complex Coacervates",
            "S Chen, ZG Wang",
            "arXiv preprint arXiv:2409.09450",
            "https://arxiv.org/pdf/2409.09450",
            )



        document.write("</ul><br>")
        document.write("<h1>2023</h1>")
        document.write("<ul>")

        add_paper("Charge Asymmetry Suppresses Coarsening Dynamics in Polyelectrolyte Complex Coacervation",
            "S Chen, ZG Wang",
            "Phys. Rev. Lett.131.218201, 2023",
            "https://link.aps.org/doi/10.1103/PhysRevLett.131.218201",
            )
  
        add_paper("Using Implicit-Solvent Potentials to Extract Water Contributions to Enthalpy–Entropy Compensation in Biomolecular Associations",
            "S Chen, ZG Wang",
            "Phys. Chem. B.127.6825-6832, 2023",
            "https://pubs.acs.org/doi/10.1021/acs.jpcb.3c03799",
            )
        add_paper("Polyelectrolyte Knot Delocalization Induced by Counterion Condensation",
            "S Chen, T Zhou",
            "arXiv preprint arXiv:2302.01080",
            "https://arxiv.org/pdf/2302.01080",
            )
  
        

        document.write("</ul><br>")
        document.write("<h1>2022</h1>")
        document.write("<ul>")
        
  
        add_paper("Driving force and pathway in polyelectrolyte complex coacervation",
            "S Chen, ZG Wang",
            "Proc. Natl. Acad. Sci. U.S.A.119 (36) e2209975119,2022",
            "https://doi.org/10.1073/pnas.2209975119",
            )

        add_paper("Complexation between Oppositely Charged Polyelectrolytes in Dilute Solution: Effects of Charge Asymmetry",
            "S Chen, P Zhang, ZG Wang",
            "Macromolecules 55.3898-3909, 2022",
            "https://pubs.acs.org/doi/full/10.1021/acs.macromol.2c00339",
            )
      
    
        add_paper("Viscoelastic Necking Dynamics Between Attractive Microgels",
            "S Chen, E Pirhadi, X Yong,",
            "Journal of Colloid and Interface Science, 2022",
            "https://doi.org/10.1016/j.jcis.2022.03.048",
            )

        document.write("</ul>")
        document.write("<h1>2021</h1>")
        document.write("<ul>")

        add_paper("Are Transformers More Robust Than CNNs?",
            "Yutong Bai, Jieru Mei, Alan Yuille, Cihang Xie",
            "NeurIPS, 2021",
            "https://arxiv.org/abs/2111.05464",
            "@inproceedings{bai2020vitsVScnns,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Are Transformers More Robust Than CNNs?},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Bai, Yutong and Mei, Jieru and Yuille, Alan and Xie, Cihang},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {NeurIPS},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
            "Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair & in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at: https://github.com/ytongbai/ViTs-vs-CNNs.",
            "https://arxiv.org/abs/2111.05464",
            "https://github.com/ytongbai/ViTs-vs-CNNs"
        )

        add_paper("Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images",
            "Zhuowan Li, Elias Stengel-Eskin, Yixiao Zhang, Cihang Xie, Quan Tran, Benjamin Van Durme, Alan Yuille",
            "ICCV, 2021",
            "https://arxiv.org/abs/2110.00519",
            "@inproceedings{li2021calico,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Calibrating Concepts and Operations: Towards Symbolic Reasoning on Real Images},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Li, Zhuowan and Stengel-Eskin, Elias and Zhang, Yixiao and Xie, Cihang and Tran, Quan and Van Durme, Benjamin and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {ICCV},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
            "While neural symbolic methods demonstrate impressive performance in visual question answering on synthetic images, their performance suffers on real images. We identify that the long-tail distribution of visual concepts and unequal importance of reasoning steps in real data are the two key obstacles that limit the models’ real-world potentials. To address these challenges, we propose a new paradigm, Calibrating Concepts and Operations (CCO), which enables neural symbolic models to capture underlying data characteristics and to reason with hierarchical importance. Specifically, we introduce an executor with learnable concept embedding magnitudes for handling distribution imbalance, and an operation calibrator for highlighting important operations and suppressing redundant ones. Ourexperiments show CCOsubstantially boosts the performance of neural symbolic methods on real images. By evaluating models on the real world dataset GQA, CCO helps the neural symbolic method NSCL outperforms its vanilla counterpart by 9.1% (from 47.0% to 56.1%); this result also largely reduces the performance gap between symbolic and non-symbolic methods. Additionally, we create a perturbed test set for better understanding and analyzing model performance on real images. Code is available at https://github.com/Lizw14/CaliCO",
            "https://arxiv.org/abs/2110.00519",
            "https://github.com/Lizw14/CaliCO"
        )

        add_paper("Robust and Accurate Object Detection via Adversarial Learning",
            "Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, Boqing Gong",
            "CVPR, 2021",
            "https://arxiv.org/abs/2103.13886",
            "@inproceedings{chen2021robust,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Robust and Accurate Object Detection via Adversarial Learning},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Chen, Xiangning and Xie, Cihang and Tan, Mingxing and Zhang, Li and Hsieh, Cho-Jui and Gong, Boqing},<br>"+
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year   = {2021}<br>}",
            "Data augmentation has become a de facto component for training high-performance deep image classifiers, but its potential is under-explored for object detection. Noting that most state-of-the-art object detectors benefit from fine-tuning a pre-trained classifier, we first study how the classifiers' gains from various data augmentations transfer to object detection. The results are discouraging; the gains diminish after fine-tuning in terms of either accuracy or robustness. This work instead augments the fine-tuning stage for object detectors by exploring adversarial examples, which can be viewed as a model-dependent data augmentation. Our method dynamically selects the stronger adversarial images sourced from a detector's classification and localization branches and evolves with the detector to ensure the augmentation policy stays current and relevant. This model-dependent augmentation generalizes to different object detectors better than AutoAugment, a model-agnostic augmentation policy searched based on one particular detector. Our approach boosts the performance of state-of-the-art EfficientDets by +1.1 mAP on the COCO object detection benchmark. It also improves the detectors' robustness against natural distortions by +3.8 mAP and against domain shift by +1.3 mAP. Models are available at https://github.com/google/automl/blob/master/efficientdet/Det-AdvProp.md",
            "https://arxiv.org/abs/2103.13886",
            "https://github.com/google/automl/blob/master/efficientdet/Det-AdvProp.md"
        )



        add_paper("Pancreas CT Segmentation by Predictive Phenotyping",
            "Yucheng Tang, Riqiang Gao, Hohin Lee, Qi Yang, Xin Yu, Yuyin Zhou, Shunxing Bao, Yuankai Huo, Jeffrey Spraggins, Jack Virostko, Zhoubing Xu, Bennett A Landman",
            "MICCAI, 2021",
            "https://link.springer.com/chapter/10.1007/978-3-030-87193-2_3",
            "@inproceedings{tang2021pancreas,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Pancreas CT Segmentation by Predictive Phenotyping},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Tang, Yucheng and Gao, Riqiang and Lee, Hohin and Yang, Qi and Yu, Xin and Zhou, Yuyin and Bao, Shunxing and Huo, Yuankai and Spraggins, Jeffrey and Virostko, Jack and others},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI},<br>" +
             "&nbsp;&nbsp;&nbsp; year     = {2021}<br>}",
            "Pancreas CT segmentation offers promise at understanding the structural manifestation of metabolic conditions. To date, the medical primary record of conditions that impact the pancreas is in the electronic health record (EHR) in terms of diagnostic phenotype data (e.g., ICD-10 codes). We posit that similar structural phenotypes could be revealed by studying subjects with similar medical outcomes. Segmentation is mainly driven by imaging data, but this direct approach may not consider differing canonical appearances with different underlying conditions (e.g., pancreatic atrophy versus pancreatic cysts). To this end, we exploit clinical features from EHR data to complement image features for enhancing the pancreas segmentation, especially in high-risk outcomes. Specifically, we propose, to the best of our knowledge, the first phenotype embedding model for pancreas segmentation by predicting representatives that share similar comorbidities. Such an embedding strategy can adaptively refine the segmentation outcome based on the discriminative contexts distilled from clinical features. Experiments with 2000 patients’ EHR data and 300 CT images with the healthy pancreas, type II diabetes, and pancreatitis subjects show that segmentation by predictive phenotyping significantly improves performance over state-of-the-arts (Dice score 0.775 to 0.791, p<0.05, Wilcoxon signed-rank test). The proposed method additionally achieves superior performance on two public testing datasets, BTCV MICCAI Challenge 2015 and TCIA pancreas CT. Our approach provides a promising direction of advancing segmentation with phenotype features while without requiring EHR data as input during testing.",
            "https://link.springer.com/chapter/10.1007/978-3-030-87193-2_3"
        ) 

         add_paper("Learning inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma prediction",
            "Yan Wang, Peng Tang, Yuyin Zhou, Wei Shen, Elliot K Fishman, Alan Yuille",
            "IEEE TMI, 2021",
            "https://arxiv.org/abs/2105.14773",
            "@article{wang2021learning,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Learning inductive attention guidance for partially supervised pancreatic ductal adenocarcinoma prediction},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Wang, Yan and Tang, Peng and Zhou, Yuyin and Shen, Wei and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {IEEE Transactions on Medical Imaging},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
            "Pancreatic ductal adenocarcinoma (PDAC) is the third most common cause of cancer death in the United States. Predicting tumors like PDACs (including both classification and segmentation) from medical images by deep learning is becoming a growing trend, but usually a large number of annotated data are required for training, which is very labor-intensive and time-consuming. In this paper, we consider a partially supervised setting, where cheap image-level annotations are provided for all the training data, and the costly per-voxel annotations are only available for a subset of them. We propose an Inductive Attention Guidance Network (IAG-Net) to jointly learn a global image-level classifier for normal/PDAC classification and a local voxel-level classifier for semi-supervised PDAC segmentation. We instantiate both the global and the local classifiers by multiple instance learning (MIL), where the attention guidance, indicating roughly where the PDAC regions are, is the key to bridging them: For global MIL based normal/PDAC classification, attention serves as a weight for each instance (voxel) during MIL pooling, which eliminates the distraction from the background; For local MIL based semi-supervised PDAC segmentation, the attention guidance is inductive, which not only provides bag-level pseudo-labels to training data without per-voxel annotations for MIL training, but also acts as a proxy of an instance-level classifier. Experimental results show that our IAG-Net boosts PDAC segmentation accuracy by more than 5% compared with the state-of-the-arts.",
            "https://arxiv.org/abs/2105.14773"
        ) 

        add_paper("External Attention Assisted Multi-Phase Splenic Vascular Injury Segmentation With Limited Data",
            "Yuyin Zhou, David Dreizin, Yan Wang, Fengze Liu, Wei Shen, Alan Yuille",
            "IEEE TMI, 2021",
            "https://arxiv.org/abs/2201.00942",
            "@article{zhou2021external,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {External Attention Assisted Multi-Phase Splenic Vascular Injury Segmentation With Limited Data},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Dreizin, David and Wang, Yan and Liu, Fengze and Shen, Wei and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {IEEE Transactions on Medical Imaging},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2021},<br>",
            "The spleen is one of the most commonly injured solid organs in blunt abdominal trauma. The development of automatic segmentation systems from multi-phase CT for splenic vascular injury can augment severity grading for improving clinical decision support and outcome prediction. However, accurate segmentation of splenic vascular injury is challenging for the following reasons: 1) Splenic vascular injury can be highly variant in shape, texture, size, and overall appearance; and 2) Data acquisition is a complex and expensive procedure that requires intensive efforts from both data scientists and radiologists, which makes large-scale well-annotated datasets hard to acquire in general. In light of these challenges, we hereby design a novel framework for multi-phase splenic vascular injury segmentation, especially with limited data. On the one hand, we propose to leverage external data to mine pseudo splenic masks as the spatial attention, dubbed external attention , for guiding the segmentation of splenic vascular injury. On the other hand, we develop a synthetic phase augmentation module, which builds upon generative adversarial networks, for populating the internal data by fully leveraging the relation between different phases. By jointly enforcing external attention and populating internal data representation during training, our proposed method outperforms other competing methods and substantially improves the popular DeepLab-v3+ baseline by more than 7% in terms of average DSC, which confirms its effectiveness.",
            "https://arxiv.org/abs/2201.00942"
        )

         add_paper("CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks",
            "Qihang Yu, Yingwei Li, Jieru Mei, Yuyin Zhou, Alan Yuille",
            "AAAI, 2021",
            "https://arxiv.org/abs/2003.12798",
            "@inproceedings{yu2021cakes,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Yu, Qihang and Li, Yingwei and Mei, Jieru and Zhou, Yuyin and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {AAAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
            "3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene understanding, such as video analysis and volumetric image recognition. However, 3D networks can easily lead to over-parameterization which incurs expensive computation cost. In this paper, we propose Channel-wise Automatic KErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard 3D convolutions into a set of economic operations (e.g., 1D, 2D convolutions). Unlike previous methods, CAKES performs channel-wise kernel shrinkage, which enjoys the following benefits: 1) enabling operations deployed in every layer to be heterogeneous, so that they can extract diverse and complementary information to benefit the learning process; and 2) allowing for an efficient and flexible replacement design, which can be generalized to both spatial-temporal and volumetric data. Further, we propose a new search space based on CAKES, so that the configuration can be determined automatically for simplifying 3D networks. CAKES shows superior performance to other methods with similar model size, and it also achieves comparable performance to state-of-the-art methods with much fewer parameters and computational costs on tasks including 3D medical imaging segmentation and video action recognition. Codes and models are available at https://github.com/yucornetto/CAKES",
            "https://arxiv.org/abs/2003.12798",
            "https://github.com/yucornetto/CAKES"
        ) 

         add_paper("Added value of deep learning-based liver parenchymal CT volumetry for predicting major arterial injury after blunt hepatic trauma: a decision tree analysis",
            "David Dreizin, Tina Chen, Yuanyuan Liang, Yuyin Zhou, Fabio Paes, Yan Wang, Alan Yuille, Patrick Roth, Kathryn Champ, Guang Li, Ashley McLenithan, Jonathan J Morrison",
            "Abdominal Radiology, 2021",
            "https://link.springer.com/article/10.1007/s00261-020-02892-x",
            "@article{dreizin2021added,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Added value of deep learning-based liver parenchymal CT volumetry for predicting major arterial injury after blunt hepatic trauma: a decision tree analysis},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Dreizin, David and Chen, Tina and Liang, Yuanyuan and Zhou, Yuyin and Paes, Fabio and Wang, Yan and Yuille, Alan and Roth, Patrick and Champ, Kathryn and Li, Guang and others},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {Abdominal Radiology},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
            "In patients presenting with blunt hepatic injury (BHI), the utility of CT for triage to hepatic angiography remains uncertain since simple binary assessment of contrast extravasation (CE) as being present or absent has only modest accuracy for major arterial injury on digital subtraction angiography (DSA). American Association for the Surgery of Trauma (AAST) liver injury grading is coarse and subjective, with limited diagnostic utility in this setting. Volumetric measurements of hepatic injury burden could improve prediction. We hypothesized that in a cohort of patients that underwent catheter-directed hepatic angiography following admission trauma CT, a deep learning quantitative visualization method that calculates % liver parenchymal disruption (the LPD index, or LPDI) would add value to CE assessment for prediction of major hepatic arterial injury (MHAI).",
            "https://link.springer.com/article/10.1007/s00261-020-02892-x"
        ) 
  
        add_paper("Shape-Texture Debiased Neural Network Training",
            "Yingwei Li, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, Cihang Xie",
            "ICLR, 2021",
            "https://arxiv.org/abs/2010.05981",
            "@inproceedings{li2021shapetexture,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Shape-Texture Debiased Neural Network Training},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Li, Yingwei and Yu, Qihang and Tan, Mingxing and Mei, Jieru and Tang, Peng and Shen, Wei and Yuille, Alan and Xie, Cihang},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {ICLR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2021}<br>}",
            "Shape and texture are two prominent and complementary cues for recognizing objects. Nonetheless, Convolutional Neural Networks are often biased towards either texture or shape, depending on the training dataset. Our ablation shows that such bias degenerates model performance. Motivated by this observation, we develop a simple algorithm for shape-texture debiased learning. To prevent models from exclusively attending on a single cue in representation learning, we augment training data with images with conflicting shape and texture information (e.g., an image of chimpanzee shape but with lemon texture) and, most importantly, provide the corresponding supervisions from shape and texture simultaneously. Experiments show that our method successfully improves model performance on several image recognition benchmarks and adversarial robustness. For example, by training on ImageNet, it helps ResNet-152 achieve substantial improvements on ImageNet (+1.2%), ImageNet-A (+5.2%), ImageNet-C (+8.3%) and Stylized-ImageNet (+11.1%), and on defending against FGSM adversarial attacker on ImageNet (+14.4%). Our method also claims to be compatible with other advanced data augmentation strategies, e.g., Mixup and CutMix. The code is available here: https://github.com/LiYingwei/ ShapeTextureDebiasedTraining.",
            "https://arxiv.org/abs/2010.05981",
            "https://github.com/LiYingwei/ShapeTextureDebiasedTraining"
        )

        document.write("</ul>")
        document.write("<h1>2020</h1>")
        document.write("<ul>")


        add_paper("Towards Robust Representation Learning and Beyond",
            "Cihang Xie",
            "Ph.D. Dissertation, Johns Hopkins University",
            "https://jscholarship.library.jhu.edu/handle/1774.2/63790",
            "@phdthesis{xie2020phd,<br>" +
             "&nbsp;&nbsp;&nbsp;title   = {Towards Robust Representation Learning and Beyond},<br>" +
             "&nbsp;&nbsp;&nbsp;author  = {Xie, Cihang},<br>" +
             "&nbsp;&nbsp;&nbsp;year    = {2020},<br>" +
             "&nbsp;&nbsp;&nbsp;school  = {The Johns Hopkins University}<br>}",
            null,
            "https://jscholarship.library.jhu.edu/handle/1774.2/63790"
        )

        add_paper("Medical Machine Intelligence: Data-efficiency and Knowledge-awareness",
            "Yuyin Zhou",
            "Ph.D. Dissertation, Johns Hopkins University",
            "https://jscholarship.library.jhu.edu/handle/1774.2/63779",
            "@phdthesis{zhou2020medical,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Medical Machine Intelligence: Data-Efficiency and Knowledge-Awareness},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020},<br>" +
             "&nbsp;&nbsp;&nbsp;school    = {The Johns Hopkins University}<br>}",
            null,
            "https://jscholarship.library.jhu.edu/handle/1774.2/63779"
        )




        add_paper("PatchAttack: A Black-box Texture-based Attack with Reinforcement Learning",
            "Chenglin Yang, Adam Kortylewski, Cihang Xie, Yinzhi Cao, Alan Yuille",
            "ECCV, 2020",
            "https://arxiv.org/abs/2004.05682",
            "@inproceedings{yang2020patchattack,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {PatchAttack: A Black-box Texture-based Attack with Reinforcement Learning},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Chenglin Yang, Adam Kortylewski, Cihang Xie, Yinzhi Cao and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {ECCV},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Patch-based attacks introduce a perceptible but localized change to the input that induces misclassification. A limitation of current patch-based black-box attacks is that they perform poorly for targeted attacks, and even for the less challenging non-targeted scenarios, they require a large number of queries. Our proposed PatchAttack is query efficient and can break models for both targeted and non-targeted attacks. PatchAttack induces misclassifications by superimposing small textured patches on the input image. We parametrize the appearance of these patches by a dictionary of class-specific textures. This texture dictionary is learned by clustering Gram matrices of feature activations from a VGG backbone. PatchAttack optimizes the position and texture parameters of each patch using reinforcement learning. Our experiments show that PatchAttack achieves >99% success rate on ImageNet for a wide range of architectures, while only manipulating 3% of the image for non-targeted attacks and 10% on average for targeted attacks. Furthermore, we show that PatchAttack circumvents state-of-the-art adversarial defense methods successfully. The code is publicly available here: https://github.com/Chenglin-Yang/PatchAttack.",
            "https://arxiv.org/abs/2004.05682",
            "https://github.com/Chenglin-Yang/PatchAttack"
        )

        add_paper("Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses",
            "Yingwei Li, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen, Alan Yuille",
            "ECCV, 2020",
            "https://arxiv.org/abs/1904.00979",
            "@inproceedings{li2019regional,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Li, Yingwei and Bai, Song and Xie, Cihang and Liao, Zhenyu and Shen, Xiaohui and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {ECCV},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "This paper focuses on learning transferable adversarial examples specifically against defense models (models to defense adversarial attacks). In particular, we show that a simple universal perturbation can fool a series of state-of-the-art defenses.Adversarial examples generated by existing attacks are generally hard to transfer to defense models. We observe the property of regional homogeneity in adversarial perturbations and suggest that the defenses are less robust to regionally homogeneous perturbations. Therefore, we propose an effective transforming paradigm and a customized gradient transformer module to transform existing perturbations into regionally homogeneous ones. Without explicitly forcing the perturbations to be universal, we observe that a well-trained gradient transformer module tends to output input-independent gradients (hence universal) benefiting from the under-fitting phenomenon. Thorough experiments demonstrate that our work significantly outperforms the prior art attacking algorithms (either image-dependent or universal ones) by an average improvement of 14.0% when attacking 9 defenses in the transfer-based attack setting. In addition to the cross-model transferability, we also verify that regionally homogeneous perturbations can well transfer across different vision tasks (attacking with the semantic segmentation task and testing on the object detection task). The code is available here: https://github.com/LiYingwei/Regional-Homogeneity.",
            "https://arxiv.org/abs/1904.00979",
            "https://github.com/LiYingwei/Regional-Homogeneity"
        )

        add_paper("Adversarial Examples Improve Image Recognition",
            "Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan Yuille, Quoc Le",
            "CVPR, 2020",
            "https://arxiv.org/abs/1911.09665",
            "@inproceedings{xie2020adversarial,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Adversarial Examples Improve Image Recognition},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7%), ImageNet-C (+6.5%), ImageNet-A (+7.0%), Stylized-ImageNet (+4.8%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images ( 3000X more than ImageNet) and 9.4X more parameters. Code and models will be made publicly available.",
            "https://arxiv.org/abs/1911.09665",
            "https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet",
            "https://medium.com/syncedreview/google-johns-hopkins-university-can-adversarial-examples-improve-image-recognition-bcb7254e2d8",
        )

        add_paper("Neural Architecture Search for Lightweight Non-Local Networks",
            "Yingwei Li, Xiaojie Jin, Jieru Mei, Xiaochen Lian, Linjie Yang, Cihang Xie, Qihang Yu, Yuyin Zhou, Song Bai, Alan Yuille",
            "CVPR, 2020",
            "https://arxiv.org/abs/2004.01961",
            "@inproceedings{li2020nas,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Neural Architecture Search for Lightweight Non-local Networks},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Li, Yingwei and Jin, Xiaojie and Mei, Jieru and Lian, Xiaochen and Yang, Linjie and Xie, Cihang and Yu, Qihang and Zhou, Yuyin and Bai, Song and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Non-Local (NL) blocks have been widely studied in various vision tasks. However, it has been rarely explored to embed the NL blocks in mobile neural networks, mainly due to the following challenges: 1) NL blocks generally have heavy computation cost which makes it difficult to be applied in applications where computational resources are limited, and 2) it is an open problem to discover an optimal configuration to embed NL blocks into mobile neural networks. We propose AutoNL to overcome the above two obstacles. Firstly, we propose a Lightweight Non-Local (LightNL) block by squeezing the transformation operations and incorporating compact features. With the novel design choices, the proposed LightNL block is 400 times computationally cheaper than its conventional counterpart without sacrificing the performance. Secondly, by relaxing the structure of the LightNL block to be differentiable during training, we propose an efficient neural architecture search algorithm to learn an optimal configuration of LightNL blocks in an end-to-end manner. Notably, using only 32 GPU hours, the searched AutoNL model achieves 77.7% top-1 accuracy on ImageNet under a typical mobile setting (350M FLOPs), significantly outperforming previous mobile models including MobileNetV2 (+5.7%), FBNet (+2.8%) and MnasNet (+2.1%). Code and models are available at https://github.com/LiYingwei/AutoNL.",
            "https://arxiv.org/abs/2004.01961",
            "https://github.com/LiYingwei/AutoNL"
        )

        add_paper("Universal Physical Camouflage Attacks on Object Detectors",
            "Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan Yuille, Changqing Zou, Ning Liu",
            "CVPR, 2020",
            "https://arxiv.org/abs/1909.04326",
            "@inproceedings{Huang2020UPC,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Universal Physical Camouflage Attacks on Object Detectors},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Lifeng Huang and Chengying Gao and Yuyin Zhou and Cihang Xie and Alan Yuille and Changqing Zou and Ning Liu},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "In this paper, we study physical adversarial attacks on object detectors in the wild. Previous works mostly craft instance-dependent perturbations only for rigid or planar objects. To this end, we propose to learn an adversarial pattern to effectively attack all instances belonging to the same object category, referred to as Universal Physical Camouflage Attack (UPC). Concretely, UPC crafts camouflage by jointly fooling the region proposal network, as well as misleading the classifier and the regressor to output errors. In order to make UPC effective for non-rigid or non-planar objects, we introduce a set of transformations for mimicking deformable properties. We additionally impose optimization constraint to make generated patterns look natural to human observers. To fairly evaluate the effectiveness of different physical-world attacks, we present the first standardized virtual database, AttackScenes, which simulates the real 3D world in a controllable and reproducible environment. Extensive experiments suggest the superiority of our proposed UPC compared with existing physical adversarial attackers not only in virtual environments (AttackScenes), but also in real-world physical environments.",
            "https://arxiv.org/abs/1909.04326",
            "https://mesunhlf.github.io/index_physical.html"
        )

        add_paper("Deep Distance Transform for Tubular Structure Segmentation in CT Scans",
            "Yan Wang, Xu Wei, Fengze Liu, Jieneng Chen, Yuyin Zhou, Wei Shen, Elliot Fishman Alan Yuille",
            "CVPR,2020",
            "https://arxiv.org/abs/1912.03383",
            "@inproceedings{wang2020deep,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Deep distance transform for tubular structure segmentation in ct scans},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Wang, Yan and Wei, Xu and Liu, Fengze and Chen, Jieneng and Zhou, Yuyin and Shen, Wei and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Tubular structure segmentation in medical images, e.g., segmenting vessels in CT scans, serves as a vital step in the use of computers to aid in screening early stages of related diseases. But automatic tubular structure segmentation in CT scans is a challenging problem, due to issues such as poor contrast, noise and complicated background. A tubular structure usually has a cylinder-like shape which can be well represented by its skeleton and cross-sectional radii (scales). Inspired by this, we propose a geometry-aware tubular structure segmentation method, Deep Distance Transform (DDT), which combines intuitions from the classical distance transform for skeletonization and modern deep segmentation networks. DDT first learns a multi-task network to predict a segmentation mask for a tubular structure and a distance map. Each value in the map represents the distance from each tubular structure voxel to the tubular structure surface. Then the segmentation mask is refined by leveraging the shape prior reconstructed from the distance map. We apply our DDT on six medical image datasets. Results show that (1) DDT can boost tubular structure segmentation performance significantly (e.g., over 13% DSC improvement for pancreatic duct segmentation), and (2) DDT additionally provides a geometrical measurement for a tubular structure, which is important for clinical diagnosis (e.g., the cross-sectional scale of a pancreatic duct can be an indicator for pancreatic cancer).",
            "https://arxiv.org/abs/1912.03383"
        )


        add_paper("A multiscale deep learning method for quantitative visualization of traumatic hemoperitoneum at CT: assessment of feasibility and comparison with subjective categorical estimation",
            "David Dreizin, Yuyin Zhou, Shuhao Fu, Yan Wang, Guang Li, Kathryn Champ, Eliot Siegel, Ze Wang, Tina Chen, Alan Yuille",
            "Radiology: Artificial Intelligence, 2020",
            "https://pubmed.ncbi.nlm.nih.gov/33330848",
            "@article{dreizin2020multiscale,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {A multiscale deep learning method for quantitative visualization of traumatic hemoperitoneum at CT: assessment of feasibility and comparison with subjective categorical estimation},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Dreizin, David and Zhou, Yuyin and Fu, Shuhao and Wang, Yan and Li, Guang and Champ, Kathryn and Siegel, Eliot and Wang, Ze and Chen, Tina and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {Radiology: Artificial Intelligence},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "To evaluate the feasibility of a multiscale deep learning algorithm for quantitative visualization and measurement of traumatic hemoperitoneum and to compare diagnostic performance for relevant outcomes with categorical estimation.",
            "https://pubmed.ncbi.nlm.nih.gov/33330848/"
        )

        add_paper("Adversarial metric attack and defense for person re-identification",
            "Song Bai, Yingwei Li, Yuyin Zhou, Qizhu Li, Philip HS Torr",
            "IEEE TPAMI, 2020",
             "https://arxiv.org/abs/1901.10650",
            "@article{bai2020adversarial,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Adversarial metric attack and defense for person re-identification},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Bai, Song and Li, Yingwei and Zhou, Yuyin and Li, Qizhu and Torr, Philip HS},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Person re-identification (re-ID) has attracted much attention recently due to its great importance in video surveillance. In general, distance metrics used to identify two person images are expected to be robust under various appearance changes. However, our work observes the extreme vulnerability of existing distance metrics to adversarial examples, generated by simply adding human-imperceptible perturbations to person images. Hence, the security danger is dramatically increased when deploying commercial re-ID systems in video surveillance. Although adversarial examples have been extensively applied for classification analysis, it is rarely studied in metric analysis like person re-identification. The most likely reason is the natural gap between the training and testing of re-ID networks, that is, the predictions of a re-ID network cannot be directly used during testing without an effective metric. In this work, we bridge the gap by proposing Adversarial Metric Attack, a parallel methodology to adversarial classification attacks. Comprehensive experiments clearly reveal the adversarial effects in re-ID systems. Meanwhile, we also present an early attempt of training a metric-preserving network, thereby defending the metric against adversarial attacks. At last, by benchmarking various adversarial settings, we expect that our work can facilitate the development of adversarial attack and defense in metric-based applications.",
            "https://arxiv.org/abs/1901.10650"
        )

        add_paper("Intriguing Properties of Adversarial Training at Scale",
            "Cihang Xie, Alan Yuille",
            "ICLR, 2020",
            "https://arxiv.org/abs/1906.03787",
            "@inproceedings{Xie2020intriguing,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Intriguing Properties of Adversarial Training at Scale},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Cihang Xie and Alan Yuille},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {ICLR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Adversarial training is one of the main defenses against adversarial attacks. In this paper, we provide the first rigorous study on diagnosing elements of large-scale adversarial training on ImageNet, which reveals two intriguing properties. First, we study the role of normalization. Batch Normalization (BN) is a crucial element for achieving state-of-the-art performance on many vision tasks, but we show it may prevent networks from obtaining strong robustness in adversarial training. One unexpected observation is that, for models trained with BN, simply removing clean images from training data largely boosts adversarial robustness, i.e., 18.3%. We relate this phenomenon to the hypothesis that clean images and adversarial images are drawn from two different domains. This two-domain hypothesis may explain the issue of BNwhentraining with amixtureof cleanandadversarial images, as estimating normalization statistics of this mixture distribution is challenging. Guided by this two-domain hypothesis, we show disentangling the mixture distribution for normalization, i.e., applying separate BNs to clean and adversarial images for statistics estimation, achieves much stronger robustness. Additionally, we find that enforcing BNs to behave consistently at training and testing can further enhance robustness. Second, we study the role of network capacity. We find our so-called “deep” networks are still shallow for the task of adversarial learning. Unlike traditional classification tasks where accuracy is only marginally improved by adding more layers to “deep” networks (e.g., ResNet-152), adversarial training exhibits a much stronger demand on deeper networks to achieve higher adversarial robustness. This robustness improvement can be observed substantially and consistently even by pushing the network capacity to an unprecedented scale, i.e., ResNet-638.",
            "https://arxiv.org/abs/1906.03787"
        )

        add_paper("Learning Transferable Adversarial Examples via Ghost Networks",
            "Yingwei Li, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan Yuille",
            "AAAI, 2020",
            "https://arxiv.org/abs/1812.03413",
            "@inproceedings{li2020learning,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Learning transferable adversarial examples via ghost networks},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Li, Yingwei and Bai, Song and Zhou, Yuyin and Xie, Cihang and Zhang, Zhishuai and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {AAAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Recent development of adversarial attacks has proven that ensemble-based methods outperform traditional, non-ensemble ones in black-box attack. However, as it is computationally prohibitive to acquire a family of diverse models, these methods achieve inferior performance constrained by the limited number of models to be ensembled.In this paper, we propose Ghost Networks to improve the transferability of adversarial examples. The critical principle of ghost networks is to apply feature-level perturbations to an existing model to potentially create a huge set of diverse models. After that, models are subsequently fused by longitudinal ensemble. Extensive experimental results suggest that the number of networks is essential for improving the transferability of adversarial examples, but it is less necessary to independently train different networks and ensemble them in an intensive aggregation way. Instead, our work can be used as a computationally cheap and easily applied plug-in to improve adversarial approaches both in single-model and multi-model attack, compatible with residual and non-residual networks. By reproducing the NeurIPS 2017 adversarial competition, our method outperforms the No.1 attack submission by a large margin, demonstrating its effectiveness and efficiency. Code is available at https://github.com/LiYingwei/ghost-network.",
            "https://arxiv.org/abs/1812.03413",
        )


        add_paper("Detecting Pancreatic Adenocarcinoma in Multi-phase CT Scans via Alignment Ensemble",
            "Yingda Xia, Qihang Yu, Wei Shen, Yuyin Zhou, Elliot Fishman",
            "MICCAI, 2020",
            "https://arxiv.org/abs/2003.08441",
            "@inproceedings{xia2020detecting,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Detecting pancreatic ductal adenocarcinoma in multi-phase CT scans via alignment ensemble},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Xia, Yingda and Yu, Qihang and Shen, Wei and Zhou, Yuyin and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers among the population. Screening for PDACs in dynamic contrast-enhanced CT is beneficial for early diagnosis. In this paper, we investigate the problem of automated detecting PDACs in multi-phase (arterial and venous) CT scans. Multiple phases provide more information than single phase, but they are unaligned and inhomogeneous in texture, making it difficult to combine cross-phase information seamlessly. We study multiple phase alignment strategies, i.e., early alignment (image registration), late alignment (high-level feature registration), and slow alignment (multi-level feature registration), and suggest an ensemble of all these alignments as a promising way to boost the performance of PDAC detection. We provide an extensive empirical evaluation on two PDAC datasets and show that the proposed alignment ensemble significantly outperforms previous state-of-the-art approaches, illustrating the strong potential for clinical use.",
            "https://arxiv.org/abs/2003.08441"
        )

        add_paper("Domain Adaptive Relational Reasoning for 3D Multi-Organ Segmentation",
            "Shuhao Fu, Yongyi Lu, Yan Wang, Yuyin Zhou, Wei Shen, Elliot Fishman, Alan Yuille",
            "MICCAI, 2020",
            "https://arxiv.org/abs/2005.09120",
            "@inproceedings{fu2020domain,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Domain adaptive relational reasoning for 3d multi-organ segmentation},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Fu, Shuhao and Lu, Yongyi and Wang, Yan and Zhou, Yuyin and Shen, Wei and Fishman, Elliot and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "In this paper, we present a novel unsupervised domain adaptation (UDA) method, named Domain Adaptive Relational Reasoning (DARR), to generalize 3D multi-organ segmentation models to medical data collected from different scanners and/or protocols (domains). Our method is inspired by the fact that the spatial relationship between internal structures in medical images is relatively fixed, e.g., a spleen is always located at the tail of a pancreas, which serves as a latent variable to transfer the knowledge shared across multiple domains. We formulate the spatial relationship by solving a jigsaw puzzle task, i.e., recovering a CT scan from its shuffled patches, and jointly train it with the organ segmentation task. To guarantee the transferability of the learned spatial relationship to multiple domains, we additionally introduce two schemes: 1) Employing a super-resolution network also jointly trained with the segmentation model to standardize medical images from different domain to a certain spatial resolution; 2) Adapting the spatial relationship for a test image by test-time jigsaw puzzle training. Experimental results show that our method improves the performance by 29.60% DSC on target datasets on average without using any data from the target domain during training.",
            "https://arxiv.org/abs/2005.09120"
        )

        add_paper("Deep learning-based quantitative visualization and measurement of extraperitoneal hematoma volumes in patients with pelvic fractures",
            "David Dreizin, Yuyin Zhou, Tina Chen, Guang Li, Alan Yuille , Ashley McLenithan, Jonathan Morrison",
            "Journal of Trauma and Acute Care Surgery, 2020",
            "https://pubmed.ncbi.nlm.nih.gov/32107356/",
            "@article{dreizin2020deep,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Deep learning-based quantitative visualization and measurement of extraperitoneal hematoma volumes in patients with pelvic fractures: potential role in personalized forecasting and decision support},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Dreizin, David and Zhou, Yuyin and Chen, Tina and Li, Guang and Yuille, Alan and McLenithan, Ashley and Morrison, Jonathan J},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {Journal of Trauma and Acute Care Surgery}<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}" ,
            "Admission CT is a widely used diagnostic tool for patients with pelvic fractures. In this pilot study, we hypothesized that pelvic hematoma volumes derived using a rapid automated deep learning-based quantitative visualization and measurement algorithm predict interventions and outcomes including a) need for angioembolization, pelvic packing, or massive transfusion, and b) in-hospital mortality.<br>We performed a single-institution retrospective analysis of 253 patients with bleeding pelvic fractures who underwent admission abdominopelvic trauma CT between 2008–2017. Included patients had hematoma volumes ≥ 30 mL, were ≥ 18 years old, and underwent contrast-enhanced CT prior to surgical or angiographic intervention. Automated pelvic hematoma volume measurements were previously derived using a deep-learning (DL) quantitative visualization and measurement algorithm through cross-validation. A composite dependent variable of need for massive transfusion, angioembolization (AE), or pelvic packing (PP) was employed as the primary endpoint. The added utility of hematoma volume was assessed by comparing the performance of multivariable models with and without hematoma volume as a predictor. AUCs as well as sensitivities, specificities, and predictive values were determined at clinically relevant thresholds. Adjusted odds ratios (OR) of automated pelvic hematoma volumes at 200 mL increments were derived.<br>Median age was 47 [IQR 29, 61], and 70% of patients were male. Median ISS was 22 [14, 36]. 94% of patients had injuries in other body regions and 73% had polytrauma (ISS ≥ 16). 33% had Tile/OTA type B and 24% had type C pelvic fractures. 109 patients underwent AE, 22 underwent PP, and 53 received massive transfusion. A total of 123 patients received all three interventions. 16 patients died during hospitalization from causes other than untreatable (AIS 6) head injury. Variables incorporated into multivariable models included age, gender, Tile/OTA grade, admission lactate, HR, and SBP. Addition of hematoma volume resulted in a significant improvement in model performance, with AUC for the composite outcome (AE, PP, or massive transfusion) increasing from 0.74 to 0.83 (p < 0.001). Adjusted unit odds more than doubled for every additional 200 mL of hematoma volume. Incraese in model AUC for mortality with incorporation of hematoma volume was not statistically significant (0.85 versus 0.90, p = 0.12).<br>Hematoma volumes measured using a rapid automated deep learning algorithm improved prediction of need for AE, PP, or massive transfusion. Simultaneous automated measurement of multiple sources of bleeding at CT could augment outcome prediction in trauma patients.",
            "https://pubmed.ncbi.nlm.nih.gov/32107356/"
        )



        add_paper("Recurrent Saliency Transformation Network for Tiny Target Segmentation in Abdominal CT Scans",
            "Lingxi Xie, Qihang Yu, Yuyin Zhou, Yan Wang, Elliot Fishman, Alan Yuille",
            "IEEE TMI, 2020",
            "https://ieeexplore.ieee.org/abstract/document/8769868",
            "@article{xie2019recurrent,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Recurrent saliency transformation network for tiny target segmentation in abdominal CT scans},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Xie, Lingxi and Yu, Qihang and Zhou, Yuyin and Wang, Yan and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {IEEE Transactions on Medical Imaging},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "We aim at segmenting a wide variety of organs, including tiny targets (e.g., adrenal gland), and neoplasms (e.g., pancreatic cyst), from abdominal CT scans. This is a challenging task in two aspects. First, some organs (e.g., the pancreas), are highly variable in both anatomy and geometry, and thus very difficult to depict. Second, the neoplasms often vary a lot in its size, shape, as well as its location within the organ. Third, the targets (organs and neoplasms) can be considerably small compared to the human body, and so standard deep networks for segmentation are often less sensitive to these targets and thus predict less accurately especially around their boundaries. In this paper, we present an end-to-end framework named recurrent saliency transformation network (RSTN) for segmenting tiny and/or variable targets. The RSTN is a coarse-to-fine approach that uses prediction from the first (coarse) stage to shrink the input region for the second (fine) stage. A saliency transformation module is inserted between these two stages so that 1) the coarse-scaled segmentation mask can be transferred as spatial weights and applied to the fine stage and 2) the gradients can be back-propagated from the loss layer to the entire network so that the two stages are optimized in a joint manner. In the testing stage, we perform segmentation iteratively to improve accuracy. In this extended journal paper, we allow a gradual optimization to improve the stability of the RSTN, and introduce a hierarchical version named H-RSTN to segment tiny and variable neoplasms such as pancreatic cysts. Experiments are performed on several CT datasets including a public pancreas segmentation dataset, our own multi-organ dataset, and a cystic pancreas dataset. In all these cases, the RSTN outperforms the baseline (a stage-wise coarse-to-fine approach) significantly. Confirmed by the radiologists in our team, these promising segmentation results can help early diagnosis of pancreatic cancer. The code and pre-trained models of our project were made available at https://github.com/198808xc/OrganSegRSTN",
            "https://ieeexplore.ieee.org/abstract/document/8769868",
            "https://github.com/198808xc/OrganSegRSTN"
        )

        document.write("</ul>")
        document.write("<h1>2019</h1>")
        document.write("<ul>")


        add_paper("Feature Denoising for Improving Adversarial Robustness",
            "Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, Kaiming He",
            "CVPR, 2019",
            "https://arxiv.org/abs/1812.03411",
            "@inproceedings{xie2019feature,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Feature denoising for improving adversarial robustness},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Xie, Cihang and Wu, Yuxin and Maaten, Laurens van der and Yuille, Alan and He, Kaiming},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Adversarial attacks to image classification systems present challenges to convolutional networks and opportunities for understanding them. This study suggests that adversarial perturbations on images lead to noise in the features constructed by these networks. Motivated by this observation, we develop new network architectures that increase adversarial robustness by performing feature denoising. Specifically, our networks contain blocks that denoise the features using non-local means or other filters; the entire networks are trained end-to-end. When combined with adversarial training, our feature denoising networks substantially improve the state-of-the-art in adversarial robustness in both white-box and black-box attack settings. On ImageNet, under 10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our method achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks, our method secures 42.6% accuracy. Our method was ranked first in Competition on Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6% classification accuracy on a secret, ImageNet-like test dataset against 48 unknown attackers, surpassing the runner-up approach by 10%. Code is available at https://github.com/facebookresearch/ImageNet-Adversarial-Training.",
            "https://arxiv.org/abs/1812.03411",
            "https://github.com/facebookresearch/ImageNet-Adversarial-Training",
        )

        add_paper("Improving Transferability of Adversarial Examples with Input Diversity",
            "Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, Alan Yuille",
            "CVPR, 2019",
            "https://arxiv.org/abs/1803.06978",
            "@inproceedings{xie2019improving,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Improving transferability of adversarial examples with input diversity},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Xie, Cihang and Zhang, Zhishuai and Zhou, Yuyin and Bai, Song and Wang, Jianyu and Ren, Zhou and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Though CNNs have achieved the state-of-the-art performance on various vision tasks, they are vulnerable to adversarial examples --- crafted by adding human-imperceptible perturbations to clean images. However, most of the existing adversarial attacks only achieve relatively low success rates under the challenging black-box setting, where the attackers have no knowledge of the model structure and parameters. To this end, we propose to improve the transferability of adversarial examples by creating diverse input patterns. Instead of only using the original images to generate adversarial examples, our method applies random transformations to the input images at each iteration. Extensive experiments on ImageNet show that the proposed attack method can generate adversarial examples that transfer much better to different networks than existing baselines. By evaluating our method against top defense solutions and official baselines from NIPS 2017 adversarial competition, the enhanced attack reaches an average success rate of 73.0%, which outperforms the top-1 attack submission in the NIPS competition by a large margin of 6.6%. We hope that our proposed attack strategy can serve as a strong benchmark baseline for evaluating the robustness of networks to adversaries and the effectiveness of different defense methods in the future. Code is available at https://github.com/cihangxie/DI-2-FGSM.",
            "https://arxiv.org/abs/1803.06978",
            "https://github.com/cihangxie/DI-2-FGSM"
        )


        add_paper("Performance of a Deep Learning Algorithm for Automated Segmentation and Quantification of Traumatic Pelvic Hematomas on CT",
            "David Dreizin, Yuyin Zhou, Yixiao Zhang, Nikki Tirada, Alan Yuille",
            "Journal of Digital Imaging, 2019",
            "https://pubmed.ncbi.nlm.nih.gov/31172331/",
            "@article{dreizin2020performance,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Performance of a deep learning algorithm for automated segmentation and quantification of traumatic pelvic hematomas on CT},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Dreizin, David and Zhou, Yuyin and Zhang, Yixiao and Tirada, Nikki and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {Journal of Digital Imaging},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2020}<br>}",
            "The volume of pelvic hematoma at CT has been shown to be the strongest independent predictor of major arterial injury requiring angioembolization in trauma victims with pelvic fractures, and also correlates with transfusion requirement and mortality. Measurement of pelvic hematomas (unopacified extraperitoneal blood accumulated from time of injury) using semi-automated seeded region growing is time-consuming and requires trained experts, precluding routine measurement at the point of care. Pelvic hematomas are markedly variable in shape and location, have irregular ill-defined margins, have low contrast with respect to viscera and muscle, and reside within anatomically distorted pelvises. Furthermore, pelvic hematomas occupy a small proportion of the entire volume of a chest, abdomen, and pelvis (C/A/P) trauma CT. The challenges are many, and no automated methods for segmentation and volumetric analysis have been described to date. Traditional approaches using fully convolutional networks result in coarse segmentations and class imbalance with suboptimal convergence. In this study, we implement a modified coarse-to-fine deep learning approach—the Recurrent Saliency Transformation Network (RSTN) for pelvic hematoma volume segmentation. RSTN previously yielded excellent results in pancreas segmentation, where low contrast with adjacent structures, small target volume, variable location, and fine contours are also problematic. We have curated a unique single-institution corpus of 253 C/A/P admission trauma CT studies in patients with bleeding pelvic fractures with manually labeled pelvic hematomas. We hypothesized that RSTN would result in sufficiently high Dice similarity coefficients to facilitate accurate and objective volumetric measurements for outcome prediction (arterial injury requiring angioembolization). Cases were separated into five combinations of training and test sets in an 80/20 split and fivefold cross-validation was performed. Dice scores in the test set were 0.71 (SD ± 0.10) using RSTN, compared to 0.49 (SD ± 0.16) using a baseline Deep Learning Tool Kit (DLTK) reference 3D U-Net architecture. Mean inference segmentation time for RSTN was 0.90 min (± 0.26). Pearson correlation between predicted and manual labels was 0.95 with p < 0.0001. Measurement bias was within 10 mL. AUC of hematoma volumes for predicting need for angioembolization was 0.81 (predicted) versus 0.80 (manual). Qualitatively, predicted labels closely followed hematoma contours and avoided muscle and displaced viscera. Further work will involve validation using a federated dataset and incorporation into a predictive model using multiple segmented features.",
            "https://pubmed.ncbi.nlm.nih.gov/31172331/"
        )

        add_paper("Lesion detection by efficiently bridging 3D context",
            "Zhishuai Zhang, Yuyin Zhou, Wei Shen, Elliot Fishman, Alan Yuille",
            "MICCAI Workshop on Machine Learning in Medical Imaging, 2019",
            "https://link.springer.com/chapter/10.1007/978-3-030-32692-0_54",
            "@inproceedings{zhang2019lesion,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Lesion detection by efficiently bridging 3D context},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhang, Zhishuai and Zhou, Yuyin and Shen, Wei and Fishman, Elliot and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI Workshop on Machine Learning in Medical Imaging},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Lesion detection in CT (computed tomography) scan images is an important yet challenging task due to the low contrast of soft tissues and similar appearance between lesion and the background. Exploiting 3D context information has been studied extensively to improve detection accuracy. However, previous methods either use a 3D CNN which usually requires a sliding window strategy to inference and only acts on local patches; or simply concatenate feature maps of independent 2D CNNs to obtain 3D context information, which is less effective to capture 3D knowledge. To address these issues, we design a hybrid detector to combine benefits from both of the above methods. We propose to build several light-weighted 3D CNNs as subnets to bridge 2D CNNs’ intermediate features, so that 2D CNNs are connected with each other which interchange 3D context information while feed-forwarding. Comprehensive experiments in DeepLesion dataset show that our method can combine 3D knowledge effectively and provide higher quality backbone features. Our detector surpasses the current state-of-the-art by a large margin with comparable speed and GPU memory consumption.",
            "https://link.springer.com/chapter/10.1007/978-3-030-32692-0_54"
        )

        add_paper("FusionNet: Incorporating Shape and Texture for Abnormality Detection in 3D Abdominal CT Scans",
            "Fengze Liu, Yuyin Zhou, Elliot Fishman, Alan Yuille",
            "MICCAI Workshop on Machine Learning in Medical Imaging, 2019",
            "https://link.springer.com/chapter/10.1007/978-3-030-32692-0_26",
            "@inproceedings{liu2019fusionnet,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {FusionNet: Incorporating Shape and Texture for Abnormality Detection in 3D Abdominal CT Scans},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Liu, Fengze and Zhou, Yuyin and Fishman, Elliot and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI Workshop on Machine Learning in Medical Imaging},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Automatic abnormality detection in abdominal CT scans can help doctors improve the accuracy and efficiency in diagnosis. In this paper we aim at detecting pancreatic ductal adenocarcinoma (PDAC), the most common pancreatic cancer. Taking the fact that the existence of tumor can affect both the shape and the texture of pancreas, we design a system to extract the shape and texture feature at the same time for detecting PDAC. In this paper we propose a two-stage method for this 3D classification task. First, we segment the pancreas into a binary mask. Second, a FusionNet is proposed to take both the binary mask and CT image as input and perform a binary classification. The optimal architecture of the FusionNet is obtained by searching a pre-defined functional space. We show that the classification results using either shape or texture information are complementary, and by fusing them with the optimized architecture, the performance improves by a large margin. Our method achieves a specificity of 97% and a sensitivity of 92% on 200 normal scans and 136 scans with PDAC.",
            "https://link.springer.com/chapter/10.1007/978-3-030-32692-0_26"
        )

        add_paper("Multi-scale attentional network for multi-focal segmentation of active bleed after pelvic fractures",
            "Yuyin Zhou, David Dreizin, Yingwei Li, Zhishuai Zhang, Yan Wang, Alan Yuille",
            "MICCAI Workshop on Machine Learning in Medical Imaging, 2019",
            "https://link.springer.com/chapter/10.1007/978-3-030-32692-0_53",
            "@inproceedings{zhou2019multi,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Multi-scale attentional network for multi-focal segmentation of active bleed after pelvic fractures},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Dreizin, David and Li, Yingwei and Zhang, Zhishuai and Wang, Yan and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI Workshop on Machine Learning in Medical Imaging},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Trauma is the worldwide leading cause of death and disability in those younger than 45 years, and pelvic fractures are a major source of morbidity and mortality. Automated segmentation of multiple foci of arterial bleeding from abdominopelvic trauma CT could provide rapid objective measurements of the total extent of active bleeding, potentially augmenting outcome prediction at the point of care, while improving patient triage, allocation of appropriate resources, and time to definitive intervention. In spite of the importance of active bleeding in the quick tempo of trauma care, the task is still quite challenging due to the variable contrast, intensity, location, size, shape, and multiplicity of bleeding foci. Existing work presents a heuristic rule-based segmentation technique which requires multiple stages and cannot be efficiently optimized end-to-end. To this end, we present, Multi-Scale Attentional Network (MSAN), the first yet reliable end-to-end network, for automated segmentation of active hemorrhage from contrast-enhanced trauma CT scans. MSAN consists of the following components: (1) an encoder which fully integrates the global contextual information from holistic 2D slices; (2) a multi-scale strategy applied both in the training stage and the inference stage to handle the challenges induced by variation of target sizes; (3) an attentional module to further refine the deep features, leading to better segmentation quality; and (4) a multi-view mechanism to leverage the 3D information. MSAN reports a significant improvement of more than 7% compared to prior arts in terms of DSC.",
            "https://link.springer.com/chapter/10.1007/978-3-030-32692-0_53"
        )

        add_paper("Application of deep learning to pancreatic cancer detection: lessons learned from our initial experience",
            "Linda C Chu, Seyoun Park, Satomi Kawamoto, Yan Wang, Yuyin Zhou, Wei Shen, Zhuotun Zhu, Yingda Xia, Lingxi Xie, Fengze Liu, Qihang Yu, Daniel F Fouladi, Shahab Shayesteh, Eva Zinreich, Jefferson S Graves, Karen M Horton, Alan Yuille, Ralph H Hruban, Kenneth W Kinzler, Bert Vogelstein, Elliot K Fishman",
            "Journal of the American College of Radiology, 2019",
            "https://www.jacr.org/article/S1546-1440(19)30631-3/abstract",
            "@article{chu2019application,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Application of deep learning to pancreatic cancer detection: lessons learned from our initial experience},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Chu, Linda C and Park, Seyoun and Kawamoto, Satomi and Wang, Yan and Zhou, Yuyin and Shen, Wei and Zhu, Zhuotun and Xia, Yingda and Xie, Lingxi and Liu, Fengze and others},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {Journal of the American College of Radiology},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Excitement has been steadily growing over the promise of artificial intelligence (AI) for radiology. Deep learning, a form of AI, uses training data and multiple layers of equations to develop a mathematical model that best fits the data [ 1 ]. The model can make predictions on the basis of new data. These algorithms deliver the prospect of improved disease detection and disease prognostication. As radiologists face increased pressure to read more cases each day, deep learning and other forms of AI offer the potential to serve as a “second reader” to decrease misses and increase efficiency. AI can analyze thousands of images on a pixel-by-pixel level and is not susceptible to mistakes due to fatigue, interruptions, or satisfaction of search.",
            "https://www.jacr.org/article/S1546-1440(19)30631-3/abstract"
        )


        add_paper("Hyper-Pairing Network for Multi-Phase Pancreatic Ductal Adenocarcinoma Segmentation",
            "Yuyin Zhou, Yingwei Li, Zhishuai Zhang, Yan Wang, Angtian Wang, Elliot K. Fishman, Alan Yuille, Seyoun Park",
            "MICCAI, 2019",
            "https://arxiv.org/abs/1909.00906",
            "@inproceedings{zhou2019hyper,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Hyper-pairing network for multi-phase pancreatic ductal adenocarcinoma segmentation},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Li, Yingwei and Zhang, Zhishuai and Wang, Yan and Wang, Angtian and Fishman, Elliot K and Yuille, Alan and Park, Seyoun},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle     = {MICCAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers with an overall five-year survival rate of 8%. Due to subtle texture changes of PDAC, pancreatic dual-phase imaging is recommended for better diagnosis of pancreatic disease. In this study, we aim at enhancing PDAC automatic segmentation by integrating multi-phase information (i.e., arterial phase and venous phase). To this end, we present Hyper-Pairing Network (HPN), a 3D fully convolution neural network which effectively integrates information from different phases. The proposed approach consists of a dual path network where the two parallel streams are interconnected with hyper-connections for intensive information exchange. Additionally, a pairing loss is added to encourage the commonality between high-level feature representations of different phases. Compared to prior arts which use single phase data, HPN reports a significant improvement up to 7.73% (from 56.21% to 63.94%) in terms of DSC.",
            "https://arxiv.org/abs/1909.00906"
        )

        add_paper("Prior-aware Neural Network for Partially-Supervised Multi-Organ Segmentation",
            "Yuyin Zhou, Zhe Li, Song Bai, Chong Wang, Xinlei Chen, Mei Han, Elliot Fishman, Alan Yuille",
            "ICCV, 2019",
            "https://arxiv.org/abs/1904.06346",
            "@inproceedings{zhou2019prior,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Prior-aware neural network for partially-supervised multi-organ segmentation},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Li, Zhe and Bai, Song and Wang, Chong and Chen, Xinlei and Han, Mei and Fishman, Elliot and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Accurate multi-organ abdominal CT segmentation is essential to many clinical applications such as computer-aided intervention. As data annotation requires massive human labor from experienced radiologists, it is common that training data are partially labeled, e.g., pancreas datasets only have the pancreas labeled while leaving the rest marked as background. However, these background labels can be misleading in multi-organ segmentation since the \"background\" usually contains some other organs of interest. To address the background ambiguity in these partially-labeled datasets, we propose Prior-aware Neural Network (PaNN) via explicitly incorporating anatomical priors on abdominal organ sizes, guiding the training process with domain-specific knowledge. More specifically, PaNN assumes that the average organ size distributions in the abdomen should approximate their empirical distributions, a prior statistics obtained from the fully-labeled dataset. As our training objective is difficult to be directly optimized using stochastic gradient descent [20], we propose to reformulate it in a min-max form and optimize it via the stochastic primal-dual gradient algorithm. PaNN achieves state-of-the-art performance on the MICCAI2015 challenge \"Multi-Atlas Labeling Beyond the Cranial Vault\", a competition on organ segmentation in the abdomen. We report an average Dice score of 84.97%, surpassing the prior art by a large margin of 3.27%.",
            "https://arxiv.org/abs/1904.06346"
        )


        add_paper("Abdominal multi-organ segmentation with organ-attention networks and statistical fusion",
            "Yan Wang, Yuyin Zhou, Wei Shen, Seyoun Park, Elliot Fishman, Alan Yuille",
            "Medical Image Analysis, 2019",
            "https://arxiv.org/abs/1804.08414",
            "@article{wang2019abdominal,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Abdominal multi-organ segmentation with organ-attention networks and statistical fusion},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Wang, Yan and Zhou, Yuyin and Shen, Wei and Park, Seyoun and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {Medical Image Analysis},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Accurate and robust segmentation of abdominal organs on CT is essential for many clinical applications such as computer-aided diagnosis and computer-aided surgery. But this task is challenging due to the weak boundaries of organs, the complexity of the background, and the variable sizes of different organs. To address these challenges, we introduce a novel framework for multi-organ segmentation of abdominal regions by using organ-attention networks with reverse connections (OAN-RCs) which are applied to 2D views, of the 3D CT volume, and output estimates which are combined by statistical fusion exploiting structural similarity. More specifically, OAN is a two-stage deep convolutional network, where deep network features from the first stage are combined with the original image, in a second stage, to reduce the complex background and enhance the discriminative information for the target organs. Intuitively, OAN reduces the effect of the complex background by focusing attention so that each organ only needs to be discriminated from its local background. RCs are added to the first stage to give the lower layers more semantic information thereby enabling them to adapt to the sizes of different organs. Our networks are trained on 2D views (slices) enabling us to use holistic information and allowing efficient computation (compared to using 3D patches). To compensate for the limited cross-sectional information of the original 3D volumetric CT, e.g., the connectivity between neighbor slices, multi-sectional images are reconstructed from the three different 2D view directions. Then we combine the segmentation results from the different views using statistical fusion, with a novel term relating the structural similarity of the 2D views to the original 3D structure. To train the network and evaluate results, 13 structures were manually annotated by four human raters and confirmed by a senior expert on 236 normal cases. We tested our algorithm by 4-fold cross-validation and computed Dice–Sørensen similarity coefficients (DSC) and surface distances for evaluating our estimates of the 13 structures. Our experiments show that the proposed approach gives strong results and outperforms 2D- and 3D-patch based state-of-the-art methods in terms of DSC and mean surface distances.",
            "https://arxiv.org/abs/1804.08414"
        )

        add_paper("Semi- Supervised 3D Multi-Organ Segmentation via Deep Multi-Planar Co-Training",
            "Yuyin Zhou, Yan Wang, Peng Tang, Song Bai, Wei Shen, Elliot Fishman, Alan Yuille",
            "WACV, 2019",
            "https://arxiv.org/abs/1804.02586",
            "@inproceedings{zhou2019semi,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Semi-supervised 3D abdominal multi-organ segmentation via deep multi-planar co-training},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Wang, Yan and Tang, Peng and Bai, Song and Shen, Wei and Fishman, Elliot and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {2019 WACV},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "In multi-organ segmentation of abdominal CT scans, most existing fully supervised deep learning algorithms require lots of voxel-wise annotations, which are usually difficult, expensive, and slow to obtain. In comparison, massive unlabeled 3D CT volumes are usually easily accessible. Current mainstream works to address semi-supervised biomedical image segmentation problem are mostly graph-based. By contrast, deep network based semi-supervised learning methods have not drawn much attention in this field. In this work, we propose Deep Multi-Planar Co-Training (DMPCT), whose contributions can be divided into two folds: 1) The deep model is learned in a co-training style which can mine consensus information from multiple planes like the sagittal, coronal, and axial planes; 2) Multi-planar fusion is applied to generate more reliable pseudo-labels, which alleviates the errors occurring in the pseudo-labels and thus can help to train better segmentation networks. Experiments are done on our newly collected large dataset with 100 unlabeled cases as well as 210 labeled cases where 16 anatomical structures are manually annotated by four radiologists and confirmed by a senior expert. The results suggest that DMPCT significantly outperforms the fully supervised method by more than 4% especially when only a small set of annotations is used.",
            "https://arxiv.org/abs/1804.02586"
        )

        add_paper("Volumetric medical image segmentation: a 3D deep coarse-to-fine framework and its adversarial examples",
            "Yingwei Li, Zhuotun Zhu, Yuyin Zhou, Yingda Xia, Wei Shen, Elliot K Fishman, Alan Yuille",
            "Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics, 2019",
            "https://arxiv.org/abs/2010.16074",
            "@incollection{li2019volumetric,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Volumetric medical image segmentation: a 3D deep coarse-to-fine framework and its adversarial examples}, <br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Li, Yingwei and Zhu, Zhuotun and Zhou, Yuyin and Xia, Yingda and Shen, Wei and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Although deep neural networks have been a dominant method for many 2D vision tasks, it is still challenging to apply them to 3D tasks, such as medical image segmentation, due to the limited amount of annotated 3D data and limited computational resources. In this chapter, by rethinking the strategy to apply 3D Convolutional Neural Networks to segment medical images, we propose a novel 3D-based coarse-to-fine framework to efficiently tackle these challenges. The proposed 3D-based framework outperforms their 2D counterparts by a large margin since it can leverage the rich spatial information along all three axes. We further analyze the threat of adversarial attacks on the proposed framework and show how to defend against the attack. We conduct experiments on three datasets, the NIH pancreas dataset, the JHMI pancreas dataset and the JHMI pathological cyst dataset, where the first two and the last one contain healthy and pathological pancreases, respectively, and achieve the current state of the art in terms of Dice-Sørensen Coefficient (DSC) on all of them. Especially, on the NIH pancreas dataset, we outperform the previous best by an average of over 2%, and the worst case is improved by 7% to reach almost 70%, which indicates the reliability of our framework in clinical applications.",
            "https://arxiv.org/abs/2010.16074"
        )


        add_paper("2D-Based Coarse-to-Fine Approaches for Small Target Segmentation in Abdominal CT Scans",
            "Yuyin Zhou, Qihang Yu, Yan Wang, Lingxi Xie, Wei Shen, Elliot K Fishman, Alan Yuille",
            "Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics, 2019",
            "https://link.springer.com/chapter/10.1007/978-3-030-13969-8_3",
            "@incollection{zhou20192d,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {2D-Based Coarse-to-Fine Approaches for Small Target Segmentation in Abdominal CT Scans},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Yu, Qihang and Wang, Yan and Xie, Lingxi and Shen, Wei and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2019}<br>}",
            "Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of small organs (e.g., pancreas) or neoplasms (e.g., pancreatic cyst) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupy a large fraction of the input volume. In this chapter, we propose two coarse-to-fine mechanisms which use prediction from the first (coarse) stage to shrink the input region for the second (fine) stage. More specifically, the two stages in the first method are trained individually in a step-wise manner, so that the entire input region and the region cropped according to the bounding box are treated separately. While the second method inserts a saliency transformation module between the two stages so that the segmentation probability map from the previous iteration can be repeatedly converted as spatial weights to the current iteration. In training, it allows joint optimization over the deep networks. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments are performed on several CT datasets, including NIH pancreas, JHMI multi-organ, and JHMI pancreatic cyst dataset. Our proposed approach gives strong results in terms of DSC.",
            "https://link.springer.com/chapter/10.1007/978-3-030-13969-8_3"
        )



        document.write("</ul>")
        document.write("<h1>2018</h1>")
        document.write("<ul>")


        add_paper("Mitigating Adversarial Effects Through Randomization",
            "Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille",
            "ICLR, 2018",
            "https://arxiv.org/abs/1711.01991",
            "@inproceedings{xie2018mitigating,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Mitigating Adversarial Effects Through Randomization},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Ren, Zhou and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {ICLR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2018}<br>}",
            "Convolutional neural networks have demonstrated high accuracy on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. For example, imperceptible perturbations added to clean images can cause convolutional neural networks to fail. In this paper, we propose to utilize randomization at inference time to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method provides the following advantages: 1) no additional training or f ine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https://github.com/cihangxie/NIPS2017_adv_challenge_defense",
            "https://arxiv.org/abs/1711.01991",
            "https://github.com/cihangxie/NIPS2017_adv_challenge_defense",
        )

        add_paper("Single-Shot Object Detection with Enriched Semantics",
            "Zhishuai Zhang, Siyuan Qiao, Cihang Xie, Wei Shen, Bo Wang, Alan Yuille",
            "CVPR, 2018",
            "https://arxiv.org/abs/1712.00433",
            "@inproceedings{zhang2018des,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Single-Shot Object Detection with Enriched Semantics},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhang, Zhishuai and Qiao, Siyuan and Xie, Cihang and Shen, Wei and Wang, Bo and Yuille, Alan.},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2018}<br>}",
            "We propose a novel single shot object detection network named Detection with Enriched Semantics (DES). Our motivation is to enrich the semantics of object detection features within a typical deep detector, by a semantic segmentation branch and a global activation module. The segmentation branch is supervised by weak segmentation ground-truth, i.e., no extra annotation is required. In conjunction with that, we employ a global activation module which learns relationship between channels and object classes in a self-supervised manner. Comprehensive experimental results on both PASCAL VOC and MS COCO detection datasets demonstrate the effectiveness of the proposed method. In particular, with a VGG16 based DES, we achieve an mAP of 81.7 on VOC2007 test and an mAP of 32.8 on COCO test-dev with an inference speed of 31.5 milliseconds per image on a Titan Xp GPU. With a lower resolution version, we achieve an mAP of 79.7 on VOC2007 with an inference speed of 13.0 milliseconds per image.",
            "https://arxiv.org/abs/1712.00433",
            "https://github.com/bairdzhang/des"
        )

        add_paper("DeepVoting: A Robust and Explainable Deep Network for Semantic Part Detection under Partial Occlusion",
            "Zhishuai Zhang, Cihang Xie, Jianyu Wang, Lingxi Xie, Alan Yuille",
            "CVPR, 2018",
            "https://arxiv.org/abs/1709.04577",
            "@inproceedings{zhang2018deepvoting,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {DeepVoting: a robust and explainable deep network for semantic part detection under partial occlusion},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhang, Zhishuai and Xie, Cihang and Wang, Jianyu and Xie, Lingxi and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {CVPR},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2018}<br>}",
            "In this paper, we study the task of detecting semantic parts of an object, e.g., a wheel of a car, under partial occlusion. We propose that all models should be trained without seeing occlusions while being able to transfer the learned knowledge to deal with occlusions. This setting alleviates the difficulty in collecting an exponentially large dataset to cover occlusion patterns and is more essential. In this scenario, the proposal-based deep networks, like RCNN-series, often produce unsatisfactory results, because both the proposal extraction and classification stages may be confused by the irrelevant occluders. To address this, [25] proposed a voting mechanism that combines multiple local visual cues to detect semantic parts. The semantic parts can still be detected even though some visual cues are missing due to occlusions. However, this method is manually-designed, thus is hard to be optimized in an end-to-end manner. In this paper, we present DeepVoting, which incorporates the robustness shown by [25] into a deep network, so that the whole pipeline can be jointly optimized. Specifically, it adds two layers after the intermediate features of a deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the evidence of local visual cues, and the second layer performs a voting mechanism by utilizing the spatial relationship between visual cues and semantic parts. We also propose an improved version DeepVoting+ by learning visual cues from context outside objects. In experiments, DeepVoting achieves significantly better performance than several baseline methods, including Faster-RCNN, for semantic part detection under occlusion. In addition, DeepVoting enjoys explainability as the detection results can be diagnosed via looking up the voting cues.",
            "https://arxiv.org/abs/1709.04577"
        )

        add_paper("Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation",
            "Qihang Yu, Lingxi Xie, Yan Wang, Yuyin Zhou, Elliot Fishman, Alan Yuille",
            "CVPR, 2018",
            "https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Recurrent_Saliency_Transformation_CVPR_2018_paper.pdf",
            "@inproceedings{yu2018recurrent,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Recurrent saliency transformation network: Incorporating multi-stage visual cues for small organ segmentation},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Yu, Qihang and Xie, Lingxi and Wang, Yan and Zhou, Yuyin and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition}, <br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2018}<br>}",
            "We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage. This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice.",
            "https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Recurrent_Saliency_Transformation_CVPR_2018_paper.pdf"
        )

         add_paper("Visual Concepts and Compositional Voting",
            "Jianyu Wang, Zhishuai Zhang, Cihang Xie, Yuyin Zhou, Vittal Premachandran, Jun Zhu, Lingxi Xie, Alan Yuille",
            "Annals of Mathematical Sciences and Applications, 2018",
            "https://arxiv.org/abs/1711.04451",
            "@article{wang2018vcsp,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Visual concepts and compositional voting},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Wang, Jianyu and Zhang, Zhishuai and Xie, Cihang and Zhou, Yuyin and Premachandran, Vittal and Zhu, Jun and Xie, Lingxi and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;journal   = {Annals of Mathematical Sciences and Applications},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2018},<br>",
            "It is very attractive to formulate vision in terms of pattern theory [26], where patterns are defined hierarchically by compositions of elementary building blocks. But applying pattern theory to real world images is currently less successful than discriminative methods such as deep networks. Deep networks, however, are black-boxes which are hard to interpret and can easily be fooled by adding occluding objects. It is natural to wonder whether by better understanding deep networks we can extract building blocks which can be used to develop pattern theoretic models. This motivates us to study the internal representations of a deep network using vehicle images from the PASCAL3D+ dataset. We use clustering algorithms to study the population activities of the features and extract a set of visual concepts which we show are visually tight and correspond to semantic parts of vehicles. To analyze this we annotate these vehicles by their semantic parts to create a new dataset, VehicleSemanticParts, and evaluate visual concepts as unsupervised part detectors. We show that visual concepts perform fairly well but are outperformed by supervised discriminative methods such as Support Vector Machines (SVM). We next give a more detailed analysis of visual concepts and how they relate to semantic parts. Following this, we use the visual concepts as building blocks for a simple pattern theoretical model, which we call compositional voting. In this model several visual concepts combine to detect semantic parts. We show that this approach is significantly better than discriminative methods like SVM and deep networks trained specifically for semantic part detection. Finally, we return to studying occlusion by creating an annotated dataset with occlusion, called VehicleOcclusion, and show that compositional voting outperforms even deep networks when the amount of occlusion becomes large.",
            "https://arxiv.org/abs/1711.04451"
        )

         add_paper("Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound",
            "Yan Wang, Yuyin Zhou, Peng Tang, Wei Shen, Elliot Fishman, Alan Yuille",
            "MICCAI, 2018",
            "https://link.springer.com/chapter/10.1007/978-3-030-00937-3_50",
            "@inproceedings{wang2018training,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Training multi-organ segmentation networks with sample selection by relaxed upper confident bound},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Wang, Yan and Zhou, Yuyin and Tang, Peng and Shen, Wei and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2018}<br>}",
            "Convolutional neural networks (CNNs), especially fully convolutional networks, have been widely applied to automatic medical image segmentation problems, e.g., multi-organ segmentation. Existing CNN-based segmentation methods mainly focus on looking for increasingly powerful network architectures, but pay less attention to data sampling strategies for training networks more effectively. In this paper, we present a simple but effective sample selection method for training multi-organ segmentation networks. Sample selection exhibits an exploitation-exploration strategy, i.e., exploiting hard samples and exploring less frequently visited samples. Based on the fact that very hard samples might have annotation errors, we propose a new sample selection policy, named Relaxed Upper Confident Bound (RUCB). Compared with other sample selection policies, e.g., Upper Confident Bound (UCB), it exploits a range of hard samples rather than being stuck with a small set of very hard ones, which mitigates the influence of annotation errors during training. We apply this new sample selection policy to training a multi-organ segmentation network on a dataset containing 120 abdominal CT scans and show that it boosts segmentation performance significantly.",
            "https://link.springer.com/chapter/10.1007/978-3-030-00937-3_50"
        )

        document.write("</ul>")
        document.write("<h1>2017</h1>")
        document.write("<ul>")

        add_paper("Adversarial Examples for Semantic Segmentation and Object Detection",
            "Jianyu Wang, Cihang Xie, Zhishuai Zhang, Jun Zhu, Lingxi Xie, Alan Yuille",
            "ICCV, 2017",
            "https://arxiv.org/abs/1703.08603",
            "@inproceedings{xie2017dag,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Adversarial Examples for Semantic Segmentation and Object Detection},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Xie, Cihang and Wang, Jianyu and Zhang, Zhishuai and Zhou, Yuyin and Xie, Lingxi and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {ICCV},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2017}<br>}",
            "It has been well demonstrated that adversarial examples, i.e., natural images with visually imperceptible perturbations added, generally exist for deep networks to fail on image classification. In this paper, we extend adversarial examples to semantic segmentation and object detection which are much more difficult. Our observation is that both segmentation and detection are based on classifying multiple targets on an image (e.g., the basic target is a pixel or a receptive field in segmentation, and an object proposal in detection), which inspires us to optimize a loss function over a set of pixels/proposals for generating adversarial perturbations. Based on this idea, we propose a novel algorithm named Dense Adversary Generation (DAG), which generates a large family of adversarial examples, and applies to a wide range of state-of-the-art deep networks for segmentation and detection. We also find that the adversarial perturbations can be transferred across networks with different training data, based on different architectures, and even for different recognition tasks. In particular, the transferability across networks with the same architecture is more significant than in other cases. Besides, summing up heterogeneous perturbations often leads to better transfer performance, which provides an effective method of black-box adversarial attack.",
            "https://arxiv.org/abs/1703.08603",
            "https://github.com/cihangxie/DAG",
        )

        add_paper("Detecting Semantic Parts on Partially Occluded Objects",
            "Jianyu Wang, Cihang Xie, Zhishuai Zhang, Jun Zhu, Lingxi Xie, Alan Yuille",
            "BMVC, 2017",
            "https://arxiv.org/abs/1707.07819",
            "@inproceedings{wang2017voting,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Detecting semantic parts on partially occluded objects},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Wang, Jianyu and Xie, Cihang and Zhang, Zhishuai and Zhu, Jun and Xie, Lingxi and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {BMVC},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2017}<br>}",
            "In this paper, we address the task of detecting semantic parts on partially occluded objects. We consider a scenario where the model is trained using non-occluded images but tested on occluded images. The motivation is that there are infinite number of occlusion patterns in real world, which cannot be fully covered in the training data. So the models should be inherently robust and adaptive to occlusions instead of fitting / learning the occlusion patterns in the training data. Our approach detects semantic parts by accumulating the confidence of local visual cues. Specifically, the method uses a simple voting method, based on log-likelihood ratio tests and spatial constraints, to combine the evidence of local cues. These cues are called visual concepts, which are derived by clustering the internal states of deep networks. We evaluate our voting scheme on the VehicleSemanticPart dataset with dense part annotations. We randomly place two, three or four irrelevant objects onto the target object to generate testing images with various occlusions. Experiments show that our algorithm outperforms several competitors in semantic part detection when occlusions are present.",
            "https://arxiv.org/abs/1707.07819"
        )

        add_paper("A Fixed- Point Model for Pancreas Segmentation in Abdominal CT Scans",
            "Yuyin Zhou, Lingxi Xie, Cihang Xie, Wei Shen, Yan Wang, Elliot Fishman, Alan Yuille",
            "MICCAI, 2017",
            "https://link.springer.com/chapter/10.1007/978-3-319-66182-7_79",
            "@inproceedings{zhou2017fixed,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {A fixed-point model for pancreas segmentation in abdominal CT scans},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Xie, Lingxi and Shen, Wei and Wang, Yan and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2017}<br>}",
            "Deep neural networks have been widely adopted for automatic organ segmentation from abdominal CT scans. However, the segmentation accuracy of some small organs (e.g., the pancreas) is sometimes below satisfaction, arguably because deep networks are easily disrupted by the complex and variable background regions which occupies a large fraction of the input volume. In this paper, we formulate this problem into a fixed-point model which uses a predicted segmentation mask to shrink the input region. This is motivated by the fact that a smaller input region often leads to more accurate segmentation. In the training process, we use the ground-truth annotation to generate accurate input regions and optimize network weights. On the testing stage, we fix the network parameters and update the segmentation results in an iterative manner. We evaluate our approach on the NIH pancreas segmentation dataset, and outperform the state-of-the-art by more than 4%, measured by the average Dice-Sørensen Coefficient (DSC). In addition, we report 62.43% DSC in the worst case, which guarantees the reliability of our approach in clinical applications.",
            "https://link.springer.com/chapter/10.1007/978-3-319-66182-7_79"
        )

        add_paper("Deep Supervision for Pancreatic Cyst Segmentation in Abdominal CT Scans",
            "Yuyin Zhou, Lingxi Xie, Elliot Fishman, Alan Yuille",
            "MICCAI, 2017",
            "https://arxiv.org/abs/1706.07346",
            "@inproceedings{zhou2017deep,<br>" +
             "&nbsp;&nbsp;&nbsp;title     = {Deep supervision for pancreatic cyst segmentation in abdominal CT scans},<br>" +
             "&nbsp;&nbsp;&nbsp;author    = {Zhou, Yuyin and Xie, Lingxi and Fishman, Elliot K and Yuille, Alan},<br>" +
             "&nbsp;&nbsp;&nbsp;booktitle = {MICCAI},<br>" +
             "&nbsp;&nbsp;&nbsp;year      = {2017}<br>}",
            "Automatic segmentation of an organ and its cystic region is a prerequisite of computer-aided diagnosis. In this paper, we focus on pancreatic cyst segmentation in abdominal CT scan. This task is important and very useful in clinical practice yet challenging due to the low contrast in boundary, the variability in location, shape and the different stages of the pancreatic cancer. Inspired by the high relevance between the location of a pancreas and its cystic region, we introduce extra deep supervision into the segmentation network, so that cyst segmentation can be improved with the help of relatively easier pancreas segmentation. Under a reasonable transformation function, our approach can be factorized into two stages, and each stage can be efficiently optimized via gradient back-propagation throughout the deep networks. We collect a new dataset with 131 pathological samples, which, to the best of our knowledge, is the largest set for pancreatic cyst segmentation. Without human assistance, our approach reports a 63.44% average accuracy, measured by the Dice-Sørensen coefficient (DSC), which is higher than the number (60.46%) without deep supervision.",
            "https://arxiv.org/abs/1706.07346"
        )

</script>

</script>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
</div>






  <!-- ======= Footer ======= -->
  <footer id="footer">

    <div class="container d-md-flex py-4">

      <div class="me-md-auto text-center text-md-start">
        <div class="copyright">
          &copy; Copyright <strong><span>CHEN-lab</span></strong>. All Rights Reserved
        </div>
        <div class="credits">
          Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
        </div>
      </div>
    </div>
  </footer><!-- End Footer -->

  <div id="preloader"></div>
  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/purecounter/purecounter.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>

